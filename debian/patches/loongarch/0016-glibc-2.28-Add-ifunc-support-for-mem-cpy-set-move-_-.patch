From 2aa1323c6eb6bc33568ae3247523eb6e32210ea7 Mon Sep 17 00:00:00 2001
From: caiyinyu <caiyinyu@loongson.cn>
Date: Thu, 9 Feb 2023 17:03:36 +0800
Subject: [PATCH 16/44] glibc-2.28: Add ifunc support for
 mem{cpy,set,move}_{aligned,unaligned,lasx}.

Change-Id: Ief0841b318d615c3a04363c0295d6fa8c2280e18
---
 sysdeps/loongarch/lp64/memcpy.S               | 1026 +++++++++++++----
 sysdeps/loongarch/lp64/memmove.S              |  478 +-------
 sysdeps/loongarch/lp64/memset.S               |  320 ++---
 sysdeps/loongarch/lp64/multiarch/Makefile     |    5 +
 .../lp64/multiarch/ifunc-impl-list.c          |   57 +
 .../loongarch/lp64/multiarch/ifunc-memcpy.h   |   37 +
 .../loongarch/lp64/multiarch/ifunc-memmove.h  |   34 +
 .../loongarch/lp64/multiarch/memcpy-aligned.S |   16 +
 .../loongarch/lp64/multiarch/memcpy-lasx.S    |  964 ++++++++++++++++
 .../lp64/multiarch/memcpy-unaligned.S         |  261 +++++
 sysdeps/loongarch/lp64/multiarch/memcpy.c     |   39 +
 .../lp64/multiarch/memmove-unaligned.S        |  480 ++++++++
 sysdeps/loongarch/lp64/multiarch/memmove.c    |   39 +
 .../loongarch/lp64/multiarch/memset-aligned.S |   11 +
 .../loongarch/lp64/multiarch/memset-lasx.S    |  330 ++++++
 .../lp64/multiarch/memset-unaligned.S         |  179 +++
 sysdeps/loongarch/lp64/multiarch/memset.c     |   39 +
 17 files changed, 3449 insertions(+), 866 deletions(-)
 create mode 100644 sysdeps/loongarch/lp64/multiarch/Makefile
 create mode 100644 sysdeps/loongarch/lp64/multiarch/ifunc-impl-list.c
 create mode 100644 sysdeps/loongarch/lp64/multiarch/ifunc-memcpy.h
 create mode 100644 sysdeps/loongarch/lp64/multiarch/ifunc-memmove.h
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memcpy-aligned.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memcpy-lasx.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memcpy-unaligned.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memcpy.c
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memmove-unaligned.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memmove.c
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memset-aligned.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memset-lasx.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memset-unaligned.S
 create mode 100644 sysdeps/loongarch/lp64/multiarch/memset.c

diff --git a/sysdeps/loongarch/lp64/memcpy.S b/sysdeps/loongarch/lp64/memcpy.S
index 5e531523f4..f699e02031 100644
--- a/sysdeps/loongarch/lp64/memcpy.S
+++ b/sysdeps/loongarch/lp64/memcpy.S
@@ -7,30 +7,55 @@
 #include <sys/asm.h>
 #endif
 
-/* Allow the routine to be named something else if desired.  */
 #ifndef MEMCPY_NAME
 #define MEMCPY_NAME memcpy
 #endif
 
-#define LD_64(reg, n) \
-	ld.d    t0, reg, n;    \
-	ld.d    t1, reg, n+8;  \
-	ld.d    t2, reg, n+16; \
-	ld.d    t3, reg, n+24; \
-	ld.d    t4, reg, n+32; \
-	ld.d    t5, reg, n+40; \
-	ld.d    t6, reg, n+48; \
-	ld.d    t7, reg, n+56;
-
-#define ST_64(reg, n) \
-	st.d    t0, reg, n;    \
-	st.d    t1, reg, n+8;  \
-	st.d    t2, reg, n+16; \
-	st.d    t3, reg, n+24; \
-	st.d    t4, reg, n+32; \
-	st.d    t5, reg, n+40; \
-	st.d    t6, reg, n+48; \
-	st.d    t7, reg, n+56;
+#ifndef MEMMOVE_NAME
+#define MEMMOVE_NAME memmove
+#endif
+
+#ifndef L
+#define L(label) .L ## label
+#endif
+
+#define LD_64(reg, n)          \
+    ld.d        t0, reg, n;    \
+    ld.d        t1, reg, n+8;  \
+    ld.d        t2, reg, n+16; \
+    ld.d        t3, reg, n+24; \
+    ld.d        t4, reg, n+32; \
+    ld.d        t5, reg, n+40; \
+    ld.d        t6, reg, n+48; \
+    ld.d        t7, reg, n+56;
+
+#define ST_64(reg, n)          \
+    st.d        t0, reg, n;    \
+    st.d        t1, reg, n+8;  \
+    st.d        t2, reg, n+16; \
+    st.d        t3, reg, n+24; \
+    st.d        t4, reg, n+32; \
+    st.d        t5, reg, n+40; \
+    st.d        t6, reg, n+48; \
+    st.d        t7, reg, n+56;
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMMOVE_NAME, 0)
+#else
+LEAF(MEMMOVE_NAME)
+#endif
+
+    .align      6
+    sub.d       t0, a0, a1
+    bltu        t0, a2, L(copy_back)
+
+END(MEMMOVE_NAME)
+
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMMOVE_NAME)
+#endif
+#endif
 
 #ifdef ANDROID_CHANGES
 LEAF(MEMCPY_NAME, 0)
@@ -38,221 +63,760 @@ LEAF(MEMCPY_NAME, 0)
 LEAF(MEMCPY_NAME)
 #endif
 
-//1st var: dst ptr: void *a1 $r4 a0
-//2nd var: src ptr: void *a2 $r5 a1
-//3rd var: size_t len $r6 a2
-//t0~t9 registers as temp
-
-	add.d   a4, a1, a2
-	add.d   a3, a0, a2
-	li.w    a6, 16
-	bge     a6, a2, less_16bytes
-	li.w    a6, 128
-	blt     a6, a2, long_bytes
-	li.w    a6, 64
-	blt     a6, a2, more_64bytes
-	li.w    a6, 32
-	blt     a6, a2, more_32bytes
-
-	/* 17...32 */
-	ld.d    t0, a1, 0
-	ld.d    t1, a1, 8
-	ld.d    t2, a4, -16
-	ld.d    t3, a4, -8
-	st.d    t0, a0, 0
-	st.d    t1, a0, 8
-	st.d    t2, a3, -16
-	st.d    t3, a3, -8
-	jr  ra
-
-more_64bytes:
-	srli.d	t8, a0, 3
-	slli.d	t8, t8, 3
-	addi.d	t8, t8,  0x8
-	sub.d	a7, a0, t8
-	ld.d	t0, a1, 0
-	sub.d	a1, a1, a7
-	st.d	t0, a0, 0
-
-	add.d	a7, a7, a2
-	addi.d	a7, a7, -0x20
-loop_32:
-	ld.d	t0, a1, 0
-	ld.d	t1, a1, 8
-	ld.d	t2, a1, 16
-	ld.d	t3, a1, 24
-	st.d	t0, t8, 0
-	st.d	t1, t8, 8
-	st.d	t2, t8, 16
-	st.d	t3, t8, 24
-
-	addi.d	t8,  t8,   0x20
-	addi.d	a1,  a1,   0x20
-	addi.d	a7,  a7,  -0x20
-	blt     zero, a7, loop_32
-
-	ld.d	t4, a4, -32
-	ld.d	t5, a4, -24
-	ld.d	t6, a4, -16
-	ld.d	t7, a4, -8
-	st.d	t4, a3, -32
-	st.d	t5, a3, -24
-	st.d	t6, a3, -16
-	st.d	t7, a3, -8
-
-	jr	ra
-
-more_32bytes:
-	/* 33...64 */
-	ld.d    t0, a1, 0
-	ld.d    t1, a1, 8
-	ld.d    t2, a1, 16
-	ld.d    t3, a1, 24
-	ld.d    t4, a4, -32
-	ld.d    t5, a4, -24
-	ld.d    t6, a4, -16
-	ld.d    t7, a4, -8
-	st.d    t0, a0, 0
-	st.d    t1, a0, 8
-	st.d    t2, a0, 16
-	st.d    t3, a0, 24
-	st.d    t4, a3, -32
-	st.d    t5, a3, -24
-	st.d    t6, a3, -16
-	st.d    t7, a3, -8
-	jr  ra
-
-less_16bytes:
-	srai.d  a6, a2, 3
-	beqz    a6, less_8bytes
-
-	/* 8...16 */
-	ld.d    t0, a1, 0
-	ld.d    t1, a4, -8
-	st.d    t0, a0, 0
-	st.d    t1, a3, -8
-
-	jr  ra
-
-less_8bytes:
-	srai.d  a6, a2, 2
-	beqz    a6, less_4bytes
-
-	/* 4...7 */
-	ld.w    t0, a1, 0
-	ld.w    t1, a4, -4
-	st.w    t0, a0, 0
-	st.w    t1, a3, -4
-	jr  ra
-
-less_4bytes:
-	srai.d  a6, a2, 1
-	beqz    a6, less_2bytes
-
-	/* 2...3 */
-	ld.h    t0, a1, 0
-	ld.h    t1, a4, -2
-	st.h    t0, a0, 0
-	st.h    t1, a3, -2
-	jr  ra
-
-less_2bytes:
-	beqz    a2, less_1bytes
-
-	ld.b    t0, a1, 0
-	st.b    t0, a0, 0
-	jr  ra
-
-less_1bytes:
-	jr  ra
-
-long_bytes:
-	srli.d  t8, a0, 3
-	slli.d  t8, t8, 3
-	beq     a0, t8, start
-
-	ld.d    t0, a1, 0
-	addi.d  t8, t8, 0x8
-	st.d    t0, a0, 0
-	sub.d   a7, a0, t8
-	sub.d   a1, a1, a7
-
-start:
-	addi.d  a5, a3, -0x80
-	blt     a5, t8, align_end_proc
-
-loop_128:
-	LD_64(a1, 0)
-	ST_64(t8, 0)
-	LD_64(a1, 64)
-	addi.d  a1, a1,  0x80
-	ST_64(t8, 64)
-	addi.d  t8, t8,  0x80
-	bge     a5, t8, loop_128
-
-align_end_proc:
-	sub.d   a2, a3, t8
-
-	pcaddi  t1, 34
-	andi    t2, a2, 0x78
-	sub.d   t1, t1, t2
-	jirl    zero, t1, 0
-
-end_120_128_unalign:
-	ld.d    t0, a1, 112
-	st.d    t0, t8, 112
-end_112_120_unalign:
-	ld.d    t0, a1, 104
-	st.d    t0, t8, 104
-end_104_112_unalign:
-	ld.d    t0, a1, 96
-	st.d    t0, t8, 96
-end_96_104_unalign:
-	ld.d    t0, a1, 88
-	st.d    t0, t8, 88
-end_88_96_unalign:
-	ld.d    t0, a1, 80
-	st.d    t0, t8, 80
-end_80_88_unalign:
-	ld.d    t0, a1, 72
-	st.d    t0, t8, 72
-end_72_80_unalign:
-	ld.d    t0, a1, 64
-	st.d    t0, t8, 64
-end_64_72_unalign:
-	ld.d    t0, a1, 56
-	st.d    t0, t8, 56
-end_56_64_unalign:
-	ld.d    t0, a1, 48
-	st.d    t0, t8, 48
-end_48_56_unalign:
-	ld.d    t0, a1, 40
-	st.d    t0, t8, 40
-end_40_48_unalign:
-	ld.d    t0, a1, 32
-	st.d    t0, t8, 32
-end_32_40_unalign:
-	ld.d    t0, a1, 24
-	st.d    t0, t8, 24
-end_24_32_unalign:
-	ld.d    t0, a1, 16
-	st.d    t0, t8, 16
-end_16_24_unalign:
-	ld.d    t0, a1, 8
-	st.d    t0, t8, 8
-end_8_16_unalign:
-	ld.d    t0, a1, 0
-	st.d    t0, t8, 0
-end_0_8_unalign:
-	ld.d    t0, a4, -8
-	st.d    t0, a3, -8
-
-	jr  ra
+    srai.d      a3, a2, 4
+    beqz        a3, L(short_data)  # less than 16 bytes
+
+    move        a4, a0
+    andi        a5, a0, 0x7
+    andi        a6, a1, 0x7
+    li.d        t8, 8
+    beqz        a5, L(check_align)
+
+    # make dest aligned 8 bytes
+    sub.d       t2, t8, a5
+    sub.d       a2, a2, t2
+
+    pcaddi      t1, 20
+    slli.d      t3, t2, 3
+    add.d       a1, a1, t2
+    sub.d       t1, t1, t3
+    add.d       a4, a4, t2
+    jr          t1
+
+L(al7):
+    ld.b        t0, a1, -7
+    st.b        t0, a4, -7
+L(al6):
+    ld.b        t0, a1, -6
+    st.b        t0, a4, -6
+L(al5):
+    ld.b        t0, a1, -5
+    st.b        t0, a4, -5
+L(al4):
+    ld.b        t0, a1, -4
+    st.b        t0, a4, -4
+L(al3):
+    ld.b        t0, a1, -3
+    st.b        t0, a4, -3
+L(al2):
+    ld.b        t0, a1, -2
+    st.b        t0, a4, -2
+L(al1):
+    ld.b        t0, a1, -1
+    st.b        t0, a4, -1
+
+L(check_align):
+    bne         a5, a6, L(unalign)
+
+    srai.d      a3, a2, 4
+    beqz        a3, L(al_less_16bytes)
+
+    andi        a3, a2, 0x3f
+    beq         a3, a2, L(al_less_64bytes)
+
+    sub.d       t0, a2, a3
+    move        a2, a3
+    add.d       a5, a1, t0
+
+L(loop_64bytes):
+    LD_64(a1, 0)
+    addi.d      a1, a1, 64
+    ST_64(a4, 0)
+
+    addi.d      a4, a4, 64
+    bne         a1, a5, L(loop_64bytes)
+
+L(al_less_64bytes):
+    srai.d     a3, a2, 5
+    beqz       a3, L(al_less_32bytes)
+
+    ld.d       t0, a1, 0
+    ld.d       t1, a1, 8
+    ld.d       t2, a1, 16
+    ld.d       t3, a1, 24
+
+    addi.d     a1, a1, 32
+    addi.d     a2, a2, -32
+
+    st.d       t0, a4, 0
+    st.d       t1, a4, 8
+    st.d       t2, a4, 16
+    st.d       t3, a4, 24
+
+    addi.d     a4, a4, 32
+
+L(al_less_32bytes):
+    srai.d     a3, a2, 4
+    beqz       a3, L(al_less_16bytes)
+
+    ld.d       t0, a1, 0
+    ld.d       t1, a1, 8
+    addi.d     a1, a1, 16
+    addi.d     a2, a2, -16
+
+    st.d       t0, a4, 0
+    st.d       t1, a4, 8
+    addi.d     a4, a4, 16
+
+L(al_less_16bytes):
+    srai.d     a3, a2, 3
+    beqz       a3, L(al_less_8bytes)
+
+    ld.d       t0, a1, 0
+    addi.d     a1, a1, 8
+    addi.d     a2, a2, -8
+
+    st.d       t0, a4, 0
+    addi.d     a4, a4, 8
+
+L(al_less_8bytes):
+    srai.d      a3, a2, 2
+    beqz        a3, L(al_less_4bytes)
+
+    ld.w        t0, a1, 0
+    addi.d      a1, a1, 4
+    addi.d      a2, a2, -4
+
+    st.w        t0, a4, 0
+    addi.d      a4, a4, 4
+
+L(al_less_4bytes):
+    srai.d      a3, a2, 1
+    beqz        a3, L(al_less_2bytes)
+
+    ld.h        t0, a1, 0
+    addi.d      a1, a1, 2
+    addi.d      a2, a2, -2
+
+    st.h        t0, a4, 0
+    addi.d      a4, a4, 2
+
+L(al_less_2bytes):
+    beqz        a2, L(al_less_1byte)
+
+    ld.b        t0, a1, 0
+    st.b        t0, a4, 0
+
+L(al_less_1byte):
+    jr          ra
+
+L(unalign):
+    andi        a5, a1, 0x7
+    bstrins.d   a1, zero, 2, 0   # make src 8 bytes aligned
+
+    sub.d       t8, t8, a5  # use t8 to save count of bytes for aligning
+    slli.d      a5, a5, 3
+
+    ld.d        t0, a1, 0
+    addi.d      a1, a1, 8
+
+    slli.d      a6, t8, 3
+    srl.d       a7, t0, a5
+
+    srai.d      a3, a2, 4
+    beqz        a3, L(un_less_16bytes)
+
+    andi        a3, a2, 0x3f
+    beq         a3, a2, L(un_less_64bytes)
+
+    sub.d       t0, a2, a3
+    move        a2, a3
+    add.d       a3, a1, t0
+
+# a5 shift right num
+# a6 shift left num
+# a7 remaining part
+L(un_long_bytes):
+    ld.d        t0, a1, 0
+    ld.d        t1, a1, 8
+    ld.d        t2, a1, 16
+    ld.d        t3, a1, 24
+
+    srl.d       t4, t0, a5
+    sll.d       t0, t0, a6
+
+    srl.d       t5, t1, a5
+    sll.d       t1, t1, a6
+
+    srl.d       t6, t2, a5
+    sll.d       t2, t2, a6
+
+    srl.d       t7, t3, a5
+    sll.d       t3, t3, a6
+
+    or          t0, a7, t0
+    or          t1, t4, t1
+    or          t2, t5, t2
+    or          t3, t6, t3
+
+    ld.d        t4, a1, 32
+    ld.d        t5, a1, 40
+    ld.d        t6, a1, 48
+    ld.d        a7, a1, 56
+
+    st.d        t0, a4, 0
+    st.d        t1, a4, 8
+    st.d        t2, a4, 16
+    st.d        t3, a4, 24
+
+    addi.d      a1, a1, 64
+
+    srl.d       t0, t4, a5
+    sll.d       t4, t4, a6
+
+    srl.d       t1, t5, a5
+    sll.d       t5, t5, a6
+
+    srl.d       t2, t6, a5
+    sll.d       t6, t6, a6
+
+    sll.d       t3, a7, a6
+    srl.d       a7, a7, a5
+
+    or          t4, t7, t4
+    or          t5, t0, t5
+    or          t6, t1, t6
+    or          t3, t2, t3
+
+    st.d        t4, a4, 32
+    st.d        t5, a4, 40
+    st.d        t6, a4, 48
+    st.d        t3, a4, 56
+
+    addi.d      a4, a4, 64
+    bne         a3, a1, L(un_long_bytes)
+
+L(un_less_64bytes):
+    srai.d	a3, a2, 5
+    beqz	a3, L(un_less_32bytes)
+
+    ld.d        t0, a1, 0
+    ld.d        t1, a1, 8
+    ld.d        t2, a1, 16
+    ld.d        t3, a1, 24
+
+    addi.d      a1, a1, 32
+    addi.d      a2, a2, -32
+
+    srl.d       t4, t0, a5
+    sll.d       t0, t0, a6
+
+    srl.d       t5, t1, a5
+    sll.d       t1, t1, a6
+
+    srl.d       t6, t2, a5
+    sll.d       t2, t2, a6
+
+    or          t0, a7, t0
+
+    srl.d       a7, t3, a5
+    sll.d       t3, t3, a6
+
+    or          t1, t4, t1
+    or          t2, t5, t2
+    or          t3, t6, t3
+
+    st.d        t0, a4, 0
+    st.d        t1, a4, 8
+    st.d        t2, a4, 16
+    st.d        t3, a4, 24
+
+    addi.d      a4, a4, 32
+
+L(un_less_32bytes):
+    srai.d      a3, a2, 4
+    beqz        a3, L(un_less_16bytes)
+
+    ld.d        t0, a1, 0
+    ld.d        t1, a1, 8
+
+    addi.d      a1, a1, 16
+    addi.d      a2, a2, -16
+
+    srl.d       t2, t0, a5
+    sll.d       t3, t0, a6
+
+    sll.d       t4, t1, a6
+    or          t3, a7, t3
+    or          t4, t2, t4
+    srl.d       a7, t1, a5
+
+    st.d        t3, a4, 0
+    st.d        t4, a4, 8
+
+    addi.d      a4, a4, 16
+
+L(un_less_16bytes):
+    srai.d      a3, a2, 3
+    beqz        a3, L(un_less_8bytes)
+
+    ld.d        t0, a1, 0
+
+    addi.d      a1, a1, 8
+    addi.d      a2, a2, -8
+
+    sll.d       t1, t0, a6
+    or          t2, a7, t1
+    srl.d       a7, t0, a5
+
+    st.d        t2, a4, 0
+    addi.d      a4, a4, 8
+
+L(un_less_8bytes):
+    beqz        a2, L(un_less_1byte)
+    bge         t8, a2, 1f # no more data in memory, un_less_8bytes data is stored in a7
+
+    # combine data in memory and a7(remaining part)
+    ld.d        t0, a1, 0
+    sll.d       t0, t0, a6
+    or          a7, a7, t0
+
+1:
+    srai.d      a3, a2, 2
+    beqz        a3, L(un_less_4bytes)
+
+    addi.d      a2, a2, -4
+    st.w        a7, a4, 0
+    addi.d      a4, a4, 4
+    srai.d      a7, a7, 32
+
+L(un_less_4bytes):
+    srai.d      a3, a2, 1
+    beqz        a3, L(un_less_2bytes)
+
+    addi.d      a2, a2, -2
+    st.h        a7, a4, 0
+    addi.d      a4, a4, 2
+    srai.d      a7, a7, 16
+
+L(un_less_2bytes):
+    beqz        a2, L(un_less_1byte)
+    st.b        a7, a4, 0
+
+L(un_less_1byte):
+    jr          ra
+
+# Bytes copying for data less than 16 bytes
+L(short_data):
+    pcaddi      t1, 36
+    slli.d      t2, a2, 3
+    add.d       a4, a0, a2
+    sub.d       t1, t1, t2
+    add.d       a1, a1, a2
+    jr          t1
+
+L(short_15_bytes):
+    ld.b       t0, a1, -15
+    st.b       t0, a4, -15
+L(short_14_bytes):
+    ld.b       t0, a1, -14
+    st.b       t0, a4, -14
+L(short_13_bytes):
+    ld.b       t0, a1, -13
+    st.b       t0, a4, -13
+L(short_12_bytes):
+    ld.b       t0, a1, -12
+    st.b       t0, a4, -12
+L(short_11_bytes):
+    ld.b       t0, a1, -11
+    st.b       t0, a4, -11
+L(short_10_bytes):
+    ld.b       t0, a1, -10
+    st.b       t0, a4, -10
+L(short_9_bytes):
+    ld.b       t0, a1, -9
+    st.b       t0, a4, -9
+L(short_8_bytes):
+    ld.b       t0, a1, -8
+    st.b       t0, a4, -8
+L(short_7_bytes):
+    ld.b       t0, a1, -7
+    st.b       t0, a4, -7
+L(short_6_bytes):
+    ld.b       t0, a1, -6
+    st.b       t0, a4, -6
+L(short_5_bytes):
+    ld.b       t0, a1, -5
+    st.b       t0, a4, -5
+L(short_4_bytes):
+    ld.b       t0, a1, -4
+    st.b       t0, a4, -4
+L(short_3_bytes):
+    ld.b       t0, a1, -3
+    st.b       t0, a4, -3
+L(short_2_bytes):
+    ld.b       t0, a1, -2
+    st.b       t0, a4, -2
+L(short_1_bytes):
+    ld.b       t0, a1, -1
+    st.b       t0, a4, -1
+    jr         ra
+
+L(copy_back):
+    srai.d      a3, a2, 4
+    beqz        a3, L(back_short_data)  # less than 16 bytes
+
+    add.d       a4, a0, a2  # store the tail of dest
+    add.d       a1, a1, a2  # store the tail of src
+
+    andi        a5, a4, 0x7
+    andi        a6, a1, 0x7
+    beqz        a5, L(back_check_align)
+
+    # make dest aligned 8 bytes
+    sub.d       a2, a2, a5
+    sub.d       a1, a1, a5
+    sub.d       a4, a4, a5
+
+    pcaddi      t1, 18
+    slli.d      t3, a5, 3
+    sub.d       t1, t1, t3
+    jr          t1
+
+    ld.b        t0, a1, 6
+    st.b        t0, a4, 6
+    ld.b        t0, a1, 5
+    st.b        t0, a4, 5
+    ld.b        t0, a1, 4
+    st.b        t0, a4, 4
+    ld.b        t0, a1, 3
+    st.b        t0, a4, 3
+    ld.b        t0, a1, 2
+    st.b        t0, a4, 2
+    ld.b        t0, a1, 1
+    st.b        t0, a4, 1
+    ld.b        t0, a1, 0
+    st.b        t0, a4, 0
+
+L(back_check_align):
+    bne         a5, a6, L(back_unalign)
+
+    srai.d      a3, a2, 4
+    beqz        a3, L(back_less_16bytes)
+
+    andi        a3, a2, 0x3f
+    beq         a3, a2, L(back_less_64bytes)
+
+    sub.d       t0, a2, a3
+    move        a2, a3
+    sub.d       a5, a1, t0
+
+L(back_loop_64bytes):
+    LD_64(a1, -64)
+    addi.d      a1, a1, -64
+    ST_64(a4, -64)
+
+    addi.d      a4, a4, -64
+    bne         a1, a5, L(back_loop_64bytes)
+
+L(back_less_64bytes):
+    srai.d     a3, a2, 5
+    beqz       a3, L(back_less_32bytes)
+
+    ld.d       t0, a1, -32
+    ld.d       t1, a1, -24
+    ld.d       t2, a1, -16
+    ld.d       t3, a1, -8
+
+    addi.d     a1, a1, -32
+    addi.d     a2, a2, -32
+
+    st.d       t0, a4, -32
+    st.d       t1, a4, -24
+    st.d       t2, a4, -16
+    st.d       t3, a4, -8
+
+    addi.d     a4, a4, -32
+
+L(back_less_32bytes):
+    srai.d     a3, a2, 4
+    beqz       a3, L(back_less_16bytes)
+
+    ld.d       t0, a1, -16
+    ld.d       t1, a1, -8
+
+    addi.d     a2, a2, -16
+    addi.d     a1, a1, -16
+
+    st.d       t0, a4, -16
+    st.d       t1, a4, -8
+    addi.d     a4, a4, -16
+
+L(back_less_16bytes):
+    srai.d      a3, a2, 3
+    beqz        a3, L(back_less_8bytes)
+
+    ld.d        t0, a1, -8
+    addi.d      a2, a2, -8
+    addi.d      a1, a1, -8
+
+    st.d        t0, a4, -8
+    addi.d      a4, a4, -8
+
+L(back_less_8bytes):
+    srai.d      a3, a2, 2
+    beqz        a3, L(back_less_4bytes)
+
+    ld.w        t0, a1, -4
+    addi.d      a2, a2, -4
+    addi.d      a1, a1, -4
+
+    st.w        t0, a4, -4
+    addi.d      a4, a4, -4
+
+L(back_less_4bytes):
+    srai.d      a3, a2, 1
+    beqz        a3, L(back_less_2bytes)
+
+    ld.h        t0, a1, -2
+    addi.d      a2, a2, -2
+    addi.d      a1, a1, -2
+
+    st.h        t0, a4, -2
+    addi.d      a4, a4, -2
+
+L(back_less_2bytes):
+    beqz        a2, L(back_less_1byte)
+
+    ld.b        t0, a1, -1
+    st.b        t0, a4, -1
+
+L(back_less_1byte):
+    jr          ra
+
+L(back_unalign):
+    andi        t8, a1, 0x7
+    bstrins.d   a1, zero, 2, 0   # make src 8 bytes aligned
+
+    sub.d       a6, zero, t8
+
+    ld.d        t0, a1, 0
+    slli.d      a6, a6, 3
+    slli.d      a5, t8, 3
+    sll.d       a7, t0, a6
+
+    srai.d      a3, a2, 4
+    beqz        a3, L(back_un_less_16bytes)
+
+    andi        a3, a2, 0x3f
+    beq         a3, a2, L(back_un_less_64bytes)
+
+    sub.d       t0, a2, a3
+    move        a2, a3
+    sub.d       a3, a1, t0
+
+L(back_un_long_bytes):
+    ld.d        t0, a1, -8
+    ld.d        t1, a1, -16
+    ld.d        t2, a1, -24
+    ld.d        t3, a1, -32
+
+    sll.d       t4, t0, a6
+    srl.d       t0, t0, a5
+
+    sll.d       t5, t1, a6
+    srl.d       t1, t1, a5
+
+    sll.d       t6, t2, a6
+    srl.d       t2, t2, a5
+
+    sll.d       t7, t3, a6
+    srl.d       t3, t3, a5
+
+    or          t0, t0, a7
+    or          t1, t1, t4
+    or          t2, t2, t5
+    or          t3, t3, t6
+
+    ld.d        t4, a1, -40
+    ld.d        t5, a1, -48
+    ld.d        t6, a1, -56
+    ld.d        a7, a1, -64
+    st.d        t0, a4, -8
+    st.d        t1, a4, -16
+    st.d        t2, a4, -24
+    st.d        t3, a4, -32
+
+    addi.d      a1, a1, -64
+
+    sll.d       t0, t4, a6
+    srl.d       t4, t4, a5
+
+    sll.d       t1, t5, a6
+    srl.d       t5, t5, a5
+
+    sll.d       t2, t6, a6
+    srl.d       t6, t6, a5
+
+    srl.d       t3, a7, a5
+    sll.d       a7, a7, a6
+
+    or          t4, t7, t4
+    or          t5, t0, t5
+    or          t6, t1, t6
+    or          t3, t2, t3
+
+    st.d        t4, a4, -40
+    st.d        t5, a4, -48
+    st.d        t6, a4, -56
+    st.d        t3, a4, -64
+
+    addi.d      a4, a4, -64
+    bne         a3, a1, L(back_un_long_bytes)
+
+L(back_un_less_64bytes):
+    srai.d	a3, a2, 5
+    beqz	a3, L(back_un_less_32bytes)
+
+    ld.d        t0, a1, -8
+    ld.d        t1, a1, -16
+    ld.d        t2, a1, -24
+    ld.d        t3, a1, -32
+
+    addi.d      a1, a1, -32
+    addi.d      a2, a2, -32
+
+    sll.d       t4, t0, a6
+    srl.d       t0, t0, a5
+
+    sll.d       t5, t1, a6
+    srl.d       t1, t1, a5
+
+    sll.d       t6, t2, a6
+    srl.d       t2, t2, a5
+
+    or          t0, a7, t0
+
+    sll.d       a7, t3, a6
+    srl.d       t3, t3, a5
+
+    or          t1, t4, t1
+    or          t2, t5, t2
+    or          t3, t6, t3
+
+    st.d        t0, a4, -8
+    st.d        t1, a4, -16
+    st.d        t2, a4, -24
+    st.d        t3, a4, -32
+
+    addi.d      a4, a4, -32
+
+L(back_un_less_32bytes):
+    srai.d      a3, a2, 4
+    beqz        a3, L(back_un_less_16bytes)
+
+    ld.d        t0, a1, -8
+    ld.d        t1, a1, -16
+
+    addi.d      a1, a1, -16
+    addi.d      a2, a2, -16
+
+    sll.d       t2, t0, a6
+    srl.d       t3, t0, a5
+
+    srl.d       t4, t1, a5
+    or          t3, a7, t3
+    or          t4, t2, t4
+    sll.d       a7, t1, a6
+
+    st.d        t3, a4, -8
+    st.d        t4, a4, -16
+
+    addi.d      a4, a4, -16
+
+L(back_un_less_16bytes):
+    srai.d      a3, a2, 3
+    beqz        a3, L(back_un_less_8bytes)
+
+    ld.d        t0, a1, -8
+
+    addi.d      a1, a1, -8
+    addi.d      a2, a2, -8
+
+    srl.d       t1, t0, a5
+    or          t2, a7, t1
+    sll.d       a7, t0, a6
+
+    st.d        t2, a4, -8
+    addi.d      a4, a4, -8
+
+L(back_un_less_8bytes):
+    beqz        a2, L(back_end)
+    bge         t8, a2, 1f # no more data in memory, un_less_8bytes data is stored in a7
+
+    # combine data in memory and a7(remaining part)
+    ld.d        t0, a1, -8
+    srl.d       t0, t0, a5
+    or          a7, a7, t0
+
+1:
+    srai.d      a3, a2, 2
+    beqz        a3, L(back_un_less_4bytes)
+
+    srai.d      t0, a7, 32
+    addi.d      a2, a2, -4
+    st.w        t0, a4, -4
+    addi.d      a4, a4, -4
+    slli.d      a7, a7, 32
+
+L(back_un_less_4bytes):
+    srai.d      a3, a2, 1
+    beqz        a3, L(back_un_less_2bytes)
+    srai.d      t0, a7, 48
+    addi.d      a2, a2, -2
+    st.h        t0, a4, -2
+    addi.d      a4, a4, -2
+    slli.d      a7, a7, 16
+L(back_un_less_2bytes):
+    beqz        a2, L(back_un_less_1byte)
+    srai.d      t0, a7, 56
+    st.b        t0, a4, -1
+L(back_un_less_1byte):
+    jr          ra
+
+L(back_short_data):
+    pcaddi     t1, 34
+    slli.d     t2, a2, 3
+    sub.d      t1, t1, t2
+    jr         t1
+
+    ld.b       t0, a1, 14
+    st.b       t0, a0, 14
+    ld.b       t0, a1, 13
+    st.b       t0, a0, 13
+    ld.b       t0, a1, 12
+    st.b       t0, a0, 12
+    ld.b       t0, a1, 11
+    st.b       t0, a0, 11
+    ld.b       t0, a1, 10
+    st.b       t0, a0, 10
+    ld.b       t0, a1, 9
+    st.b       t0, a0, 9
+    ld.b       t0, a1, 8
+    st.b       t0, a0, 8
+    ld.b       t0, a1, 7
+    st.b       t0, a0, 7
+    ld.b       t0, a1, 6
+    st.b       t0, a0, 6
+    ld.b       t0, a1, 5
+    st.b       t0, a0, 5
+    ld.b       t0, a1, 4
+    st.b       t0, a0, 4
+    ld.b       t0, a1, 3
+    st.b       t0, a0, 3
+    ld.b       t0, a1, 2
+    st.b       t0, a0, 2
+    ld.b       t0, a1, 1
+    st.b       t0, a0, 1
+    ld.b       t0, a1, 0
+    st.b       t0, a0, 0
+L(back_end):
+    jr         ra
 
 END(MEMCPY_NAME)
+
 #ifndef ANDROID_CHANGES
 #ifdef _LIBC
 libc_hidden_builtin_def (MEMCPY_NAME)
 #endif
 #endif
-
diff --git a/sysdeps/loongarch/lp64/memmove.S b/sysdeps/loongarch/lp64/memmove.S
index f87d036bcf..6d1922c464 100644
--- a/sysdeps/loongarch/lp64/memmove.S
+++ b/sysdeps/loongarch/lp64/memmove.S
@@ -1,476 +1,2 @@
-#ifdef _LIBC
-#include <sysdep.h>
-#include <sys/regdef.h>
-#include <sys/asm.h>
-#else
-#include <regdef.h>
-#include <sys/asm.h>
-#endif
-
-/* Allow the routine to be named something else if desired.  */
-#ifndef MEMMOVE_NAME
-#define MEMMOVE_NAME memmove
-#endif
-
-#define LD_64(reg, n) \
-	ld.d	t0, reg, n;    \
-	ld.d	t1, reg, n+8;  \
-	ld.d	t2, reg, n+16; \
-	ld.d	t3, reg, n+24; \
-	ld.d	t4, reg, n+32; \
-	ld.d	t5, reg, n+40; \
-	ld.d	t6, reg, n+48; \
-	ld.d	t7, reg, n+56;
-
-
-#define ST_64(reg, n) \
-	st.d	t0, reg, n;    \
-	st.d	t1, reg, n+8;  \
-	st.d	t2, reg, n+16; \
-	st.d	t3, reg, n+24; \
-	st.d	t4, reg, n+32; \
-	st.d	t5, reg, n+40; \
-	st.d	t6, reg, n+48; \
-	st.d	t7, reg, n+56;
-
-#define LDST_1024 \
-	LD_64(a1, 0);    \
-	ST_64(a0, 0);    \
-	LD_64(a1, 64);   \
-	ST_64(a0, 64);   \
-	LD_64(a1, 128);  \
-	ST_64(a0, 128);  \
-	LD_64(a1, 192);  \
-	ST_64(a0, 192);  \
-	LD_64(a1, 256);  \
-	ST_64(a0, 256);  \
-	LD_64(a1, 320);  \
-	ST_64(a0, 320);  \
-	LD_64(a1, 384);  \
-	ST_64(a0, 384);  \
-	LD_64(a1, 448);  \
-	ST_64(a0, 448);  \
-	LD_64(a1, 512);  \
-	ST_64(a0, 512);  \
-	LD_64(a1, 576);  \
-	ST_64(a0, 576);  \
-	LD_64(a1, 640);  \
-	ST_64(a0, 640);  \
-	LD_64(a1, 704);  \
-	ST_64(a0, 704);  \
-	LD_64(a1, 768);  \
-	ST_64(a0, 768);  \
-	LD_64(a1, 832);  \
-	ST_64(a0, 832);  \
-	LD_64(a1, 896);  \
-	ST_64(a0, 896);  \
-	LD_64(a1, 960);  \
-	ST_64(a0, 960);
-
-#define LDST_1024_BACK \
-	LD_64(a4, -64);   \
-	ST_64(a3, -64);   \
-	LD_64(a4, -128);  \
-	ST_64(a3, -128);  \
-	LD_64(a4, -192);  \
-	ST_64(a3, -192);  \
-	LD_64(a4, -256);  \
-	ST_64(a3, -256);  \
-	LD_64(a4, -320);  \
-	ST_64(a3, -320);  \
-	LD_64(a4, -384);  \
-	ST_64(a3, -384);  \
-	LD_64(a4, -448);  \
-	ST_64(a3, -448);  \
-	LD_64(a4, -512);  \
-	ST_64(a3, -512);  \
-	LD_64(a4, -576);  \
-	ST_64(a3, -576);  \
-	LD_64(a4, -640);  \
-	ST_64(a3, -640);  \
-	LD_64(a4, -704);  \
-	ST_64(a3, -704);  \
-	LD_64(a4, -768);  \
-	ST_64(a3, -768);  \
-	LD_64(a4, -832);  \
-	ST_64(a3, -832);  \
-	LD_64(a4, -896);  \
-	ST_64(a3, -896);  \
-	LD_64(a4, -960);  \
-	ST_64(a3, -960);  \
-	LD_64(a4, -1024); \
-	ST_64(a3, -1024);
-
-#ifdef ANDROID_CHANGES
-LEAF(MEMMOVE_NAME, 0)
-#else
-LEAF(MEMMOVE_NAME)
-#endif
-
-//1st var: dest ptr: void *str1 $r4 a0
-//2nd var: src  ptr: void *str2 $r5 a1
-//3rd var: size_t num
-//t0~t9 registers as temp
-
-	add.d	a4, a1, a2
-	add.d	a3, a0, a2
-	beq		a1, a0, less_1bytes
-	move	t8, a0
-	srai.d	a6, a2, 4  		#num/16
-	beqz	a6, less_16bytes        #num<16
-	srai.d	a6, a2, 6  		#num/64
-	bnez	a6, more_64bytes       #num>64
-	srai.d	a6, a2, 5
-	beqz	a6, less_32bytes	   #num<32
-
-	ld.d	t0, a1, 0              #32<num<64
-	ld.d	t1, a1, 8
-	ld.d	t2, a1, 16
-	ld.d	t3, a1, 24
-	ld.d	t4, a4, -32
-	ld.d	t5, a4, -24
-	ld.d	t6, a4, -16
-	ld.d	t7, a4, -8
-    st.d	t0, a0, 0
-    st.d	t1, a0, 8
-    st.d	t2, a0, 16
-    st.d	t3, a0, 24
-    st.d	t4, a3, -32
-    st.d	t5, a3, -24
-    st.d	t6, a3, -16
-    st.d	t7, a3, -8
-
-	jr  ra
-
-less_32bytes:
-	ld.d	t0, a1, 0
-	ld.d	t1, a1, 8
-	ld.d	t2, a4, -16
-	ld.d	t3, a4, -8
-	st.d	t0, a0, 0
-	st.d	t1, a0, 8
-	st.d	t2, a3, -16
-	st.d	t3, a3, -8
-
-	jr	ra
-
-less_16bytes:
-	srai.d	a6, a2, 3 #num/8
-	beqz	a6, less_8bytes
-
-	ld.d	t0, a1, 0
-	ld.d	t1, a4, -8
-	st.d	t0, a0, 0
-	st.d	t1, a3, -8
-
-	jr	ra
-
-less_8bytes:
-	srai.d	a6, a2, 2
-	beqz	a6, less_4bytes
-
-	ld.w	t0, a1, 0
-	ld.w	t1, a4, -4
-	st.w	t0, a0, 0
-	st.w	t1, a3, -4
-
-	jr	ra
-
-less_4bytes:
-	srai.d	a6, a2, 1
-	beqz	a6, less_2bytes
-
-	ld.h	t0, a1, 0
-	ld.h	t1, a4, -2
-	st.h	t0, a0, 0
-	st.h	t1, a3, -2
-
-	jr	ra
-
-less_2bytes:
-	beqz	a2, less_1bytes
-
-	ld.b	t0, a1, 0
-	st.b	t0, a0, 0
-
-	jr	ra
-
-less_1bytes:
-	jr	ra
-
-more_64bytes:
-	sub.d   a7, a0, a1
-	bltu	a7, a2, copy_backward
-
-copy_forward:
-	srli.d	a0, a0, 3
-	slli.d	a0, a0, 3
-	beq 	a0, t8, all_align
-	addi.d	a0, a0, 0x8
-	sub.d	a7, t8, a0
-	sub.d	a1, a1, a7
-	add.d	a2, a7, a2
-
-start_unalign_proc:
-	pcaddi  t1, 18
-	slli.d  a6, a7, 3
-	add.d   t1, t1, a6
-	jirl    zero, t1, 0
-
-start_7_unalign:
-	ld.b    t0, a1, -7
-	st.b    t0, a0, -7
-start_6_unalign:
-	ld.b    t0, a1, -6
-	st.b    t0, a0, -6
-start_5_unalign:
-	ld.b    t0, a1, -5
-	st.b    t0, a0, -5
-start_4_unalign:
-	ld.b    t0, a1, -4
-	st.b    t0, a0, -4
-start_3_unalign:
-	ld.b    t0, a1, -3
-	st.b    t0, a0, -3
-start_2_unalign:
-	ld.b    t0, a1, -2
-	st.b    t0, a0, -2
-start_1_unalign:
-	ld.b    t0, a1, -1
-	st.b    t0, a0, -1
-start_over:
-
-	addi.d	a2, a2, -0x80
-	blt     a2, zero, end_unalign_proc
-
-loop_less:
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0,  0x80
-	addi.d	a1, a1,  0x80
-	addi.d	a2, a2, -0x80
-	bge     a2, zero, loop_less
-
-end_unalign_proc:
-		addi.d  a2, a2, 0x80
-
-    	pcaddi  t1, 36
-    	andi    t2, a2, 0x78
-		add.d   a1, a1, t2
-		add.d   a0, a0, t2
-    	sub.d   t1, t1, t2
-    	jirl    zero, t1, 0
-
-end_120_128_unalign:
-		ld.d    t0, a1, -120
-		st.d    t0, a0, -120
-end_112_120_unalign:
-		ld.d    t0, a1, -112
-		st.d    t0, a0, -112
-end_104_112_unalign:
-		ld.d    t0, a1, -104
-		st.d    t0, a0, -104
-end_96_104_unalign:
-		ld.d    t0, a1, -96
-		st.d    t0, a0, -96
-end_88_96_unalign:
-		ld.d    t0, a1, -88
-		st.d    t0, a0, -88
-end_80_88_unalign:
-		ld.d    t0, a1, -80
-		st.d    t0, a0, -80
-end_72_80_unalign:
-		ld.d    t0, a1, -72
-		st.d    t0, a0, -72
-end_64_72_unalign:
-		ld.d    t0, a1, -64
-		st.d    t0, a0, -64
-end_56_64_unalign:
-		ld.d    t0, a1, -56
-		st.d    t0, a0, -56
-end_48_56_unalign:
-		ld.d    t0, a1, -48
-		st.d    t0, a0, -48
-end_40_48_unalign:
-		ld.d    t0, a1, -40
-		st.d    t0, a0, -40
-end_32_40_unalign:
-		ld.d    t0, a1, -32
-		st.d    t0, a0, -32
-end_24_32_unalign:
-    	ld.d    t0, a1, -24
-    	st.d    t0, a0, -24
-end_16_24_unalign:
-    	ld.d    t0, a1, -16
-    	st.d    t0, a0, -16
-end_8_16_unalign:
-    	ld.d    t0, a1, -8
-    	st.d    t0, a0, -8
-end_0_8_unalign:
-
-    	andi    a2, a2, 0x7
-		pcaddi  t1, 18
-		slli.d  a2, a2, 3
-		sub.d   t1, t1, a2
-		jirl    zero, t1, 0
-
-end_7_unalign:
-		ld.b    t0, a4, -7
-		st.b    t0, a3, -7
-end_6_unalign:
-		ld.b    t0, a4, -6
-		st.b    t0, a3, -6
-end_5_unalign:
-		ld.b    t0, a4, -5
-		st.b    t0, a3, -5
-end_4_unalign:
-		ld.b    t0, a4, -4
-		st.b    t0, a3, -4
-end_3_unalign:
-		ld.b    t0, a4, -3
-		st.b    t0, a3, -3
-end_2_unalign:
-		ld.b    t0, a4, -2
-		st.b    t0, a3, -2
-end_1_unalign:
-		ld.b    t0, a4, -1
-		st.b    t0, a3, -1
-end:
-
-		move    v0, t8
-		jr	ra
-
-all_align:
-	addi.d  a1, a1, 0x8
-	addi.d  a0, a0, 0x8
-	ld.d	t0, a1, -8
-	st.d    t0, a0, -8
-	addi.d  a2, a2, -8
-	b 		start_over
-
-all_align_back:
-	addi.d  a4, a4, -0x8
-	addi.d  a3, a3, -0x8
-	ld.d    t0, a4, 0
-	st.d    t0, a3, 0
-	addi.d  a2, a2, -8
-	b       start_over_back
-
-copy_backward:
-	move    a5, a3
-	srli.d  a3, a3, 3
-	slli.d  a3, a3, 3
-	beq     a3, a5, all_align_back
-	sub.d   a7, a3, a5
-	add.d   a4, a4, a7
-	add.d   a2, a7, a2
-
-	pcaddi  t1, 18
-	slli.d  a6, a7, 3
-	add.d   t1, t1, a6
-	jirl    zero, t1, 0
-
-	ld.b    t0, a4, 6
-	st.b    t0, a3, 6
-	ld.b    t0, a4, 5
-	st.b    t0, a3, 5
-	ld.b    t0, a4, 4
-	st.b    t0, a3, 4
-	ld.b    t0, a4, 3
-	st.b    t0, a3, 3
-	ld.b    t0, a4, 2
-	st.b    t0, a3, 2
-	ld.b    t0, a4, 1
-	st.b    t0, a3, 1
-	ld.b    t0, a4, 0
-	st.b    t0, a3, 0
-start_over_back:
-
-	addi.d  a2, a2, -0x80
-	blt     a2, zero, end_unalign_proc_back
-
-loop_less_back:
-	LD_64(a4, -64)
-	ST_64(a3, -64)
-	LD_64(a4, -128)
-	ST_64(a3, -128)
-
-	addi.d a4, a4, -0x80
-	addi.d a3, a3, -0x80
-	addi.d a2, a2, -0x80
-	bge    a2, zero, loop_less_back
-
-end_unalign_proc_back:
-		addi.d  a2, a2, 0x80
-
-		pcaddi  t1, 36
-		andi    t2, a2, 0x78
-		sub.d   a4, a4, t2
-		sub.d   a3, a3, t2
-		sub.d   t1, t1, t2
-		jirl    zero, t1, 0
-
-		ld.d    t0, a4, 112
-		st.d    t0, a3, 112
-		ld.d    t0, a4, 104
-		st.d    t0, a3, 104
-		ld.d    t0, a4, 96
-		st.d    t0, a3, 96
-		ld.d    t0, a4, 88
-		st.d    t0, a3, 88
-		ld.d    t0, a4, 80
-		st.d    t0, a3, 80
-		ld.d    t0, a4, 72
-		st.d    t0, a3, 72
-		ld.d    t0, a4, 64
-		st.d    t0, a3, 64
-		ld.d    t0, a4, 56
-		st.d    t0, a3, 56
-		ld.d    t0, a4, 48
-		st.d    t0, a3, 48
-		ld.d    t0, a4, 40
-		st.d    t0, a3, 40
-		ld.d    t0, a4, 32
-		st.d    t0, a3, 32
-    	ld.d    t0, a4, 24
-    	st.d    t0, a3, 24
-    	ld.d    t0, a4, 16
-    	st.d    t0, a3, 16
-    	ld.d    t0, a4, 8
-    	st.d    t0, a3, 8
-		ld.d    t0, a4, 0
-		st.d    t0, a3, 0
-
-		andi    a2, a2, 0x7
-		pcaddi  t1, 18
-		slli.d  a2, a2, 3
-		sub.d   t1, t1, a2
-		jirl    zero, t1, 0
-
-		ld.b    t0, a1, 6
-		st.b    t0, a0, 6
-		ld.b    t0, a1, 5
-		st.b    t0, a0, 5
-		ld.b    t0, a1, 4
-		st.b    t0, a0, 4
-		ld.b    t0, a1, 3
-		st.b    t0, a0, 3
-		ld.b    t0, a1, 2
-		st.b    t0, a0, 2
-		ld.b    t0, a1, 1
-		st.b    t0, a0, 1
-		ld.b    t0, a1, 0
-		st.b    t0, a0, 0
-
-		move    v0, t8
-		jr	ra
-
-END(MEMMOVE_NAME)
-#ifndef ANDROID_CHANGES
-#ifdef _LIBC
-libc_hidden_builtin_def (MEMMOVE_NAME)
-#endif
-#endif
+/* DONT DELETE THIS FILE, OTHERWIES MEMCPY.C WILL BE COMPILED. */
+/* There are too many common code in memcpy and memmove. See memcpy.S */
diff --git a/sysdeps/loongarch/lp64/memset.S b/sysdeps/loongarch/lp64/memset.S
index 8bc152ee22..c0063f4463 100644
--- a/sysdeps/loongarch/lp64/memset.S
+++ b/sysdeps/loongarch/lp64/memset.S
@@ -7,169 +7,171 @@
 #include <sys/regdef.h>
 #endif
 
-#ifdef LOONGSON_TEST
-#define MEMSET	_memset
-#else
-#define MEMSET	memset
+#ifndef MEMSET_NAME
+#define MEMSET_NAME memset
+#endif
+
+#ifndef L
+#define L(label) .L ## label
 #endif
 
-#define ST_128(n) 	\
-	st.d	a1, a0, n;		 \
-	st.d    a1, a0, n+8  ; 	 \
-	st.d    a1, a0, n+16 ;   \
-	st.d    a1, a0, n+24 ;   \
-	st.d    a1, a0, n+32 ;   \
-	st.d    a1, a0, n+40 ;   \
-	st.d    a1, a0, n+48 ;   \
-	st.d    a1, a0, n+56 ;   \
-	st.d    a1, a0, n+64 ;   \
-	st.d    a1, a0, n+72 ;   \
-	st.d    a1, a0, n+80 ;   \
-	st.d    a1, a0, n+88 ;   \
-	st.d    a1, a0, n+96 ;   \
-	st.d    a1, a0, n+104;   \
-	st.d    a1, a0, n+112;   \
-	st.d    a1, a0, n+120;	 \
-
-//1st var: void *str  $4 a0
-//2nd var: int val  $5   a1
-//3rd var: size_t num  $6  a2
-
-LEAF(MEMSET)
-
-memset:
-	.align	6
-
-	bstrins.d a1, a1, 15, 8
-	add.d	  t7, a0, a2
-	bstrins.d a1, a1, 31, 16
-	move	  t0, a0
-	bstrins.d a1, a1, 63, 32
-	srai.d	  t8, a2, 4         	#num/16
-	beqz	  t8, less_16bytes	#num<16
-	srai.d	  t8, a2, 6		#num/64
-	bnez	  t8, more_64bytes	#num>64
-	srai.d	  t8, a2, 5		#num/32
-	beqz	  t8, less_32bytes	#num<32
-	st.d	  a1, a0, 0 		#32<num<64
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, t7, -32
-	st.d	  a1, t7, -24
-	st.d	  a1, t7, -16
-	st.d	  a1, t7, -8
-
-	jr	  ra
-
-less_32bytes:
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, t7, -16
-	st.d	  a1, t7, -8
-
-	jr	  ra
-
-less_16bytes:
-	srai.d	  t8, a2, 3		#num/8
-	beqz	  t8, less_8bytes
-	st.d	  a1, a0, 0
-	st.d	  a1, t7, -8
-
-	jr	  ra
-
-less_8bytes:
-	srai.d	  t8, a2, 2
-	beqz	  t8, less_4bytes
-	st.w	  a1, a0, 0
-	st.w	  a1, t7, -4
-
-	jr	  ra
-
-less_4bytes:
-	srai.d	  t8, a2, 1
-	beqz	  t8, less_2bytes
-	st.h	  a1, a0, 0
-	st.h	  a1, t7, -2
-
-	jr	  ra
-
-less_2bytes:
-	beqz	  a2, less_1bytes
-	st.b	  a1, a0, 0
-
-	jr	  ra
-
-less_1bytes:
-	jr	  ra
-
-more_64bytes:
-	srli.d	  a0, a0, 3
-	slli.d	  a0, a0, 3
-	addi.d	  a0, a0, 0x8
-	st.d      a1, t0, 0
-	sub.d	  t2, t0, a0
-	add.d	  a2, t2, a2
-
-	addi.d	  a2, a2, -0x80
-	blt       a2, zero, end_unalign_proc
-
-loop_less:
-	ST_128(0)
-	addi.d	a0, a0,  0x80
-	addi.d	a2, a2, -0x80
-	bge     a2, zero, loop_less
-
-end_unalign_proc:
-	addi.d  a2, a2, 0x80
-
-	pcaddi  t1, 20
-	andi    t5, a2, 0x78
-	srli.d  t5, t5, 1
-	sub.d   t1, t1, t5
-	jirl    zero, t1, 0
-
-end_120_128_unalign:
-	st.d    a1, a0, 112
-end_112_120_unalign:
-	st.d    a1, a0, 104
-end_104_112_unalign:
-	st.d    a1, a0, 96
-end_96_104_unalign:
-	st.d    a1, a0, 88
-end_88_96_unalign:
-	st.d    a1, a0, 80
-end_80_88_unalign:
-	st.d    a1, a0, 72
-end_72_80_unalign:
-	st.d    a1, a0, 64
-end_64_72_unalign:
-	st.d    a1, a0, 56
-end_56_64_unalign:
-	st.d    a1, a0, 48
-end_48_56_unalign:
-	st.d    a1, a0, 40
-end_40_48_unalign:
-	st.d    a1, a0, 32
-end_32_40_unalign:
-	st.d    a1, a0, 24
-end_24_32_unalign:
-    st.d    a1, a0, 16
-end_16_24_unalign:
-    st.d    a1, a0, 8
-end_8_16_unalign:
-    st.d    a1, a0, 0
-end_0_8_unalign:
-
-	st.d    a1, t7, -8
-
-	move	  v0, t0
-	jr	  ra
-
-END(MEMSET)
+#define ST_64(n)                \
+    st.d        a1, a0, n;      \
+    st.d        a1, a0, n+8;    \
+    st.d        a1, a0, n+16;   \
+    st.d        a1, a0, n+24;   \
+    st.d        a1, a0, n+32;   \
+    st.d        a1, a0, n+40;   \
+    st.d        a1, a0, n+48;   \
+    st.d        a1, a0, n+56;
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMSET_NAME, 0)
+#else
+LEAF(MEMSET_NAME)
+#endif
+    .align          6
+    move        t0, a0
+    andi        a3, a0, 0x7
+    li.w        t6, 16
+    beqz        a3, L(align)
+    blt         a2, t6, L(short_data)
+
+L(make_align):
+    li.w        t8, 8
+    sub.d       t2, t8, a3
+    pcaddi      t1, 11
+    slli.d      t3, t2, 2
+    sub.d       t1, t1, t3
+    jirl        zero, t1, 0
+
+L(al7):
+    st.b        a1, t0, 6
+L(al6):
+    st.b        a1, t0, 5
+L(al5):
+    st.b        a1, t0, 4
+L(al4):
+    st.b        a1, t0, 3
+L(al3):
+    st.b        a1, t0, 2
+L(al2):
+    st.b        a1, t0, 1
+L(al1):
+    st.b        a1, t0, 0
+L(al0):
+    add.d       t0, t0, t2
+    sub.d       a2, a2, t2
+
+L(align):
+    bstrins.d   a1, a1, 15, 8
+    bstrins.d   a1, a1, 31, 16
+    bstrins.d   a1, a1, 63, 32
+
+    blt         a2, t6, L(less_16bytes)
+
+    andi        a4, a2, 0x3f
+    beq         a4, a2, L(less_64bytes)
+
+    sub.d       t1, a2, a4
+    move        a2, a4
+    add.d       a5, t0, t1
+
+L(loop_64bytes):
+    addi.d      t0, t0, 64
+    st.d        a1, t0, -64
+    st.d        a1, t0, -56
+    st.d        a1, t0, -48
+    st.d        a1, t0, -40
+    st.d        a1, t0, -32
+    st.d        a1, t0, -24
+    st.d        a1, t0, -16
+    st.d        a1, t0, -8
+    bne         t0, a5, L(loop_64bytes)
+
+L(less_64bytes):
+    srai.d      a4, a2, 5
+    beqz        a4, L(less_32bytes)
+    addi.d      a2, a2, -32
+    st.d        a1, t0, 0
+    st.d        a1, t0, 8
+    st.d        a1, t0, 16
+    st.d        a1, t0, 24
+    addi.d      t0, t0, 32
+L(less_32bytes):
+    blt         a2, t6, L(less_16bytes)
+    addi.d      a2, a2, -16
+    st.d        a1, t0, 0
+    st.d        a1, t0, 8
+    addi.d      t0, t0, 16
+L(less_16bytes):
+    srai.d      a4, a2, 3
+    beqz        a4, L(less_8bytes)
+    addi.d      a2, a2, -8
+    st.d        a1, t0, 0
+    addi.d      t0, t0, 8
+L(less_8bytes):
+    beqz        a2, L(less_1byte)
+    srai.d      a4, a2, 2
+    beqz        a4, L(less_4bytes)
+    addi.d      a2, a2, -4
+    st.w        a1, t0, 0
+    addi.d      t0, t0, 4
+L(less_4bytes):
+    srai.d      a3, a2, 1
+    beqz        a3, L(less_2bytes)
+    addi.d      a2, a2, -2
+    st.h        a1, t0, 0
+    addi.d      t0, t0, 2
+L(less_2bytes):
+    beqz        a2, L(less_1byte)
+    st.b        a1, t0, 0
+L(less_1byte):
+    jr          ra
+
+L(short_data):
+    pcaddi      t1, 19
+    slli.d      t3, a2, 2
+    sub.d       t1, t1, t3
+    jirl        zero, t1, 0
+L(short_15):
+    st.b        a1, a0, 14
+
+L(short_14):
+    st.b        a1, a0, 13
+L(short_13):
+    st.b        a1, a0, 12
+L(short_12):
+    st.b        a1, a0, 11
+L(short_11):
+    st.b        a1, a0, 10
+L(short_10):
+    st.b        a1, a0, 9
+L(short_9):
+    st.b        a1, a0, 8
+L(short_8):
+    st.b        a1, a0, 7
+L(short_7):
+    st.b        a1, a0, 6
+L(short_6):
+    st.b        a1, a0, 5
+L(short_5):
+    st.b        a1, a0, 4
+L(short_4):
+    st.b        a1, a0, 3
+L(short_3):
+    st.b        a1, a0, 2
+L(short_2):
+    st.b        a1, a0, 1
+L(short_1):
+    st.b        a1, a0, 0
+L(short_0):
+    jr          ra
+
+END(MEMSET_NAME)
 
 #ifndef ANDROID_CHANGES
 #ifdef _LIBC
-libc_hidden_builtin_def (memset)
+libc_hidden_builtin_def (MEMSET_NAME)
 #endif
 #endif
diff --git a/sysdeps/loongarch/lp64/multiarch/Makefile b/sysdeps/loongarch/lp64/multiarch/Makefile
new file mode 100644
index 0000000000..09f5fa1c42
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/Makefile
@@ -0,0 +1,5 @@
+ifeq ($(subdir),string)
+sysdep_routines += memcpy-aligned memcpy-unaligned memcpy-lasx \
+		   memset-aligned memset-unaligned memset-lasx \
+		   memmove-unaligned
+endif
diff --git a/sysdeps/loongarch/lp64/multiarch/ifunc-impl-list.c b/sysdeps/loongarch/lp64/multiarch/ifunc-impl-list.c
new file mode 100644
index 0000000000..e575118439
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/ifunc-impl-list.c
@@ -0,0 +1,57 @@
+/* Enumerate available IFUNC implementations of a function.  LoongArch64 version.
+   Copyright (C) 2017-2018 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <http://www.gnu.org/licenses/>.  */
+
+#include <assert.h>
+#include <string.h>
+#include <wchar.h>
+#include <ldsodefs.h>
+#include <ifunc-impl-list.h>
+#include <init-arch.h>
+#include <stdio.h>
+
+/* Maximum number of IFUNC implementations.  */
+#define MAX_IFUNC	3
+
+size_t
+__libc_ifunc_impl_list (const char *name, struct libc_ifunc_impl *array,
+			size_t max)
+{
+  assert (max >= MAX_IFUNC);
+
+  size_t i = 0;
+
+  IFUNC_IMPL (i, name, memcpy,
+	      IFUNC_IMPL_ADD (array, i, memcpy, 1, __memcpy_lasx)
+	      IFUNC_IMPL_ADD (array, i, memcpy, 1, __memcpy_aligned)
+	      IFUNC_IMPL_ADD (array, i, memcpy, 1, __memcpy_unaligned)
+	      )
+
+  IFUNC_IMPL (i, name, memmove,
+	      IFUNC_IMPL_ADD (array, i, memmove, 1, __memmove_aligned)
+	      IFUNC_IMPL_ADD (array, i, memmove, 1, __memmove_unaligned)
+	      )
+
+  IFUNC_IMPL (i, name, memset,
+	      IFUNC_IMPL_ADD (array, i, memset, 1, __memset_lasx)
+	      IFUNC_IMPL_ADD (array, i, memset, 1, __memset_aligned)
+	      IFUNC_IMPL_ADD (array, i, memset, 1, __memset_unaligned)
+	      )
+
+  return i;
+}
+
diff --git a/sysdeps/loongarch/lp64/multiarch/ifunc-memcpy.h b/sysdeps/loongarch/lp64/multiarch/ifunc-memcpy.h
new file mode 100644
index 0000000000..234f636be9
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/ifunc-memcpy.h
@@ -0,0 +1,37 @@
+/* Common definition for memcpy, and memset implementation.
+   All versions must be listed in ifunc-impl-list.c.
+   Copyright (C) 2017-2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <https://www.gnu.org/licenses/>.  */
+
+#include <init-arch.h>
+
+extern __typeof (REDIRECT_NAME) OPTIMIZE (lasx) attribute_hidden;
+extern __typeof (REDIRECT_NAME) OPTIMIZE (aligned) attribute_hidden;
+extern __typeof (REDIRECT_NAME) OPTIMIZE (unaligned) attribute_hidden;
+
+static inline void *
+IFUNC_SELECTOR (void)
+{
+  INIT_ARCH();
+
+  if (SUPPORT_LASX)
+    return OPTIMIZE (lasx);
+  else if (SUPPORT_UAL)
+    return OPTIMIZE (unaligned);
+  else
+    return OPTIMIZE (aligned);
+}
diff --git a/sysdeps/loongarch/lp64/multiarch/ifunc-memmove.h b/sysdeps/loongarch/lp64/multiarch/ifunc-memmove.h
new file mode 100644
index 0000000000..a6854fe966
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/ifunc-memmove.h
@@ -0,0 +1,34 @@
+/* Common definition for memcpy, and memset implementation.
+   All versions must be listed in ifunc-impl-list.c.
+   Copyright (C) 2017-2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <https://www.gnu.org/licenses/>.  */
+
+#include <init-arch.h>
+
+extern __typeof (REDIRECT_NAME) OPTIMIZE (aligned) attribute_hidden;
+extern __typeof (REDIRECT_NAME) OPTIMIZE (unaligned) attribute_hidden;
+
+static inline void *
+IFUNC_SELECTOR (void)
+{
+  INIT_ARCH();
+
+  if (SUPPORT_UAL)
+    return OPTIMIZE (unaligned);
+  else
+    return OPTIMIZE (aligned);
+}
diff --git a/sysdeps/loongarch/lp64/multiarch/memcpy-aligned.S b/sysdeps/loongarch/lp64/multiarch/memcpy-aligned.S
new file mode 100644
index 0000000000..eb56e13e33
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memcpy-aligned.S
@@ -0,0 +1,16 @@
+
+
+#if IS_IN (libc)
+
+#ifndef MEMCPY_NAME
+#define MEMCPY_NAME __memcpy_aligned
+#endif
+
+#ifndef MEMMOVE_NAME
+#define MEMMOVE_NAME __memmove_aligned
+#endif
+
+#endif
+
+#include "../memcpy.S"
+
diff --git a/sysdeps/loongarch/lp64/multiarch/memcpy-lasx.S b/sysdeps/loongarch/lp64/multiarch/memcpy-lasx.S
new file mode 100644
index 0000000000..32c3aabb86
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memcpy-lasx.S
@@ -0,0 +1,964 @@
+/* Multiple versions of memcpy. LoongArch64 version.
+   Copyright (C) 2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library.  If not, see
+   <https://www.gnu.org/licenses/>.  */
+
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <regdef.h>
+#include <sys/asm.h>
+#endif
+
+#if IS_IN (libc)
+
+/* Allow the routine to be named something else if desired.  */
+#ifndef MEMCPY_NAME
+#define MEMCPY_NAME __memcpy_lasx
+#endif
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMCPY_NAME, 0)
+#else
+LEAF(MEMCPY_NAME)
+#endif
+
+#define XVXOR_V(xd,xj,xk)       .word(0x1d<<26|0x09<<21|0x0E<<15|(xk&0x1f)<<10|(xj&0x1f)<<5|(xd&0x1f))
+#define XVREPLVE0_B(xd,xj)      .word(0x1d<<26|0x18<<21|0x0E<<15|0x0<<10|(xj&0x1f)<<5|(xd&0x1f))
+#define XVST(xd,rj,si12)        .word(0x0b<<26|0x3<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(xd&0x1f))
+#define XVLD(xd,rj,si12)        .word(0x0b<<26|0x2<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(xd&0x1f))
+#define VST(vd,rj,si12)         .word(0x0b<<26|0x1<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VLD(vd,rj,si12)         .word(0x0b<<26|0x0<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_D(vd,rj,si8,idx) .word(0x31<<24|0x2<<19|(idx&0x1)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_W(vd,rj,si8,idx) .word(0x31<<24|0x2<<20|(idx&0x3)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_H(vd,rj,si8,idx) .word(0x31<<24|0x2<<21|(idx&0x7)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_B(vd,rj,si8,idx) .word(0x31<<24|0x2<<22|(idx&0xf)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VLDREPL_D(vd,rj,si9)    .word(0x30<<24|0x2<<19|(si9&0x1ff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VLDREPL_W(vd,rj,si10)   .word(0x30<<24|0x2<<20|(si10&0x3ff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VLDREPL_H(vd,rj,si11)   .word(0x30<<24|0x2<<21|(si11&0x7ff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VLDREPL_B(vd,rj,si12)   .word(0x30<<24|0x2<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+
+/* 1st var: dest ptr: void *str1 $r4.
+ * 2nd var: src  ptr: void *str2 $r5.
+ * 3rd var: size_t num.  */
+
+	add.d	t3, a1, a2		# src end, 2st var, 3rd var t3->$r15
+	add.d	t2, a0, a2		# dest end, 1st var, 3rd var t2->$r14
+	move	t0, a0			# $2 is func return value
+	srai.d	t8, a2, 4		# num/16
+	beqz	t8, less_16bytes       	# num<16
+	nop
+	srai.d	t8, a2, 8		# num/256
+	bnez	t8, eqormore_256bytes  	# num>256
+	nop
+	srai.d	t8, a2, 7
+	beqz	t8, less_128bytes	# num<128
+	nop
+
+	XVLD(0, 5, 0)
+	XVLD(1, 5, 32)
+	XVLD(2, 5, 64)
+	XVLD(3, 5, 96)
+	XVLD(4, 15, -128)
+	XVLD(5, 15, -96)
+	XVLD(6, 15, -64)
+	XVLD(7, 15, -32)
+	XVST(0, 4, 0)
+	XVST(1, 4, 32)
+	XVST(2, 4, 64)
+	XVST(3, 4, 96)
+	XVST(4, 14, -128)
+	XVST(5, 14, -96)
+	XVST(6, 14, -64)
+	XVST(7, 14, -32)
+
+	jr	ra
+	nop
+
+less_128bytes:
+	srai.d	t8, a2, 6 		# num/64
+	beqz	t8, less_64bytes
+	nop
+
+	XVLD(0, 5, 0)
+	XVLD(1, 5, 32)
+	XVLD(6, 15, -64)
+	XVLD(7, 15, -32)
+	XVST(0, 4, 0)
+	XVST(1, 4, 32)
+	XVST(6, 14, -64)
+	XVST(7, 14, -32)
+
+	jr	ra
+	nop
+
+less_64bytes:
+	srai.d	t8, a2, 5 		# num/32
+	beqz	t8, less_32bytes
+	nop
+
+	XVLD(0, 5, 0)
+	XVLD(7, 15, -32)
+	XVST(0, 4, 0)
+	XVST(7, 14, -32)
+
+	jr	ra
+	nop
+
+less_32bytes:
+	VLD(0, 5, 0)
+	VLD(7, 15, -16)
+	VST(0, 4, 0)
+	VST(7, 14, -16)
+
+	jr	ra
+	nop
+
+less_16bytes:
+	srai.d	t8, a2, 3 		# num/8
+	beqz	t8, less_8bytes
+	nop
+	VLDREPL_D(0, 5, 0)
+	VLDREPL_D(7, 15, -1)
+	VSTELM_D (0, 4, 0, 0)      	# store lower 8bytes to mem
+	VSTELM_D (7, 14, -1, 0)	   	# store lower 8bytes to mem
+	jr	ra
+	nop
+
+less_8bytes:
+	srai.d	t8, a2, 2
+	beqz	t8, less_4bytes
+	nop
+	VLDREPL_W(0, 5, 0)
+	VLDREPL_W(7, 15, -1)
+	VSTELM_W (0, 4,  0, 0)
+	VSTELM_W (7, 14, -1, 0)
+	jr	ra
+	nop
+
+less_4bytes:
+	srai.d	t8, a2, 1
+	beqz	t8, less_2bytes
+	nop
+	VLDREPL_H(0, 5, 0)
+	VLDREPL_H(7, 15, -1)
+	VSTELM_H (0, 4,  0, 0)
+	VSTELM_H (7, 14, -1, 0)
+	jr	ra
+	nop
+
+less_2bytes:
+	beqz	a2, less_1bytes
+	nop
+	VLDREPL_B(0, 5, 0)
+	VSTELM_B (0, 4,  0,  0)
+	jr	ra
+	nop
+
+less_1bytes:
+	jr	ra
+	nop
+
+
+eqormore_256bytes:
+	srli.d	a0, a0, 5
+	slli.d	a0, a0, 5
+	addi.d	a0, a0,  0x20	 # a0:align dest start addr
+	sub.d	t7, t0,  a0      # $2:dest start addr
+
+	XVLD(0, 5, 0)
+
+	sub.d	a1,  a1,  t7     # a1:newer src
+
+	XVST(0, 12, 0)
+	XVLD(19, 15, -128)
+	XVLD(18, 15,  -96)
+	XVLD(17, 15,  -64)
+	XVLD(16, 15,  -32)
+
+	add.d	t7, t7, a2   	# t7:num
+	addi.d	t7, t7, -0x80
+	srai.d	t8, t7, 22
+	bnez	t8, loop_most	# >4MB; scache_size_half * 1/2
+	nop
+	srai.d	t8, t7, 12
+	beqz	t8, loop_less	# <4096
+	nop
+
+loop_more:
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #1
+	addi.d	a1,  a1,   512   #1
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #2
+	addi.d	a1,  a1,   512   #2
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #3
+	addi.d	a1,  a1,   512   #3
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #4
+	addi.d	a1,  a1,   512   #4
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #5
+	addi.d	a1,  a1,   512   #5
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #6
+	addi.d	a1,  a1,   512   #6
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #7
+	addi.d	a1,  a1,   512   #7
+
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+
+	addi.d	a0,  a0,   512   #8
+	addi.d	a1,  a1,   512   #8
+	addi.d	t8, t8, -1
+	bnez	t8, loop_more
+	nop
+
+  	lu12i.w t4, 1        #get imm 1<<12
+  	addi.d	t4, t4, -1
+  	and	t7, t7, t4
+
+	b	loop_less
+	nop
+
+loop_most:
+	srai.d	t8, t7, 12
+        lu12i.w t5, 1	     # load imm 4096
+        add.d 	t6, t5, a0   #t6 = a0 + 4096
+        add.d 	t5, t5, a1   #t5 = a1 + 4096
+
+loop_most_loop:
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0       #imm in pref inst is 16b, but 12b in preld.
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64      #prefech for next loop cache data
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #1
+	addi.d	a1,  a1,   512   #1
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #2
+	addi.d	a1,  a1,   512   #2
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #3
+	addi.d	a1,  a1,   512   #3
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #4
+	addi.d	a1,  a1,   512   #4
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #5
+	addi.d	a1,  a1,   512   #5
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #6
+	addi.d	a1,  a1,   512   #6
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #7
+	addi.d	a1,  a1,   512   #7
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	preld 0, t5, 0
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	preld 0, t5, 64
+	XVLD( 4, 5, 128)
+	XVLD( 5, 5, 160)
+	preld 0, t5, 128
+	XVLD( 6, 5, 192)
+	XVLD( 7, 5, 224)
+	preld 0, t5, 192
+	XVLD( 8, 5, 256)
+	XVLD( 9, 5, 288)
+	preld 0, t5, 256
+	XVLD(10, 5, 320)
+	XVLD(11, 5, 352)
+	preld 0, t5, 320
+	XVLD(12, 5, 384)
+	XVLD(13, 5, 416)
+	preld 0, t5, 384
+	XVLD(14, 5, 448)
+	XVLD(15, 5, 480)
+	preld 0, t5, 448
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	preld 8, t6, 0
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	preld 8, t6, 64
+	XVST( 4, 4, 128)
+	XVST( 5, 4, 160)
+	preld 8, t6, 128
+	XVST( 6, 4, 192)
+	XVST( 7, 4, 224)
+	preld 8, t6, 192
+	XVST( 8, 4, 256)
+	XVST( 9, 4, 288)
+	preld 8, t6, 256
+	XVST(10, 4, 320)
+	XVST(11, 4, 352)
+	preld 8, t6, 320
+	XVST(12, 4, 384)
+	XVST(13, 4, 416)
+	preld 8, t6, 384
+	XVST(14, 4, 448)
+	XVST(15, 4, 480)
+	preld 8, t6, 448
+	addi.d	a0,  a0,   512   #8
+	addi.d	a1,  a1,   512   #8
+	addi.d  t5,  t5,   512
+   	addi.d  t6,  t6,   512
+	addi.d	t8, t8, -1
+	bnez	t8, loop_most_loop
+	nop
+
+  	lu12i.w t4, 1		 #get imm 1<<12
+  	addi.d	t4, t4, -1
+  	and	t7, t7, t4
+
+loop_less:
+	XVLD( 0, 5, 0)
+	XVLD( 1, 5, 32)
+	XVLD( 2, 5, 64)
+	XVLD( 3, 5, 96)
+	XVST( 0, 4, 0)
+	XVST( 1, 4, 32)
+	XVST( 2, 4, 64)
+	XVST( 3, 4, 96)
+	addi.d	a0,  a0,  0x80
+	addi.d	a1,  a1,  0x80
+	addi.d	t7, t7,	  -0x80
+	slt	t8, t7,	  zero
+	beqz	t8, loop_less
+	nop
+	XVST(19, 14, -128)
+	XVST(18, 14, -96)
+	XVST(17, 14, -64)
+	XVST(16, 14, -32)
+
+	move	v0, t0
+	jr	ra
+	nop
+
+END(MEMCPY_NAME)
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMCPY_NAME)
+#endif
+#endif
+
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memcpy-unaligned.S b/sysdeps/loongarch/lp64/multiarch/memcpy-unaligned.S
new file mode 100644
index 0000000000..69b0a0a6bf
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memcpy-unaligned.S
@@ -0,0 +1,261 @@
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <regdef.h>
+#include <sys/asm.h>
+#endif
+
+#if IS_IN (libc)
+
+#ifndef MEMCPY_NAME
+#define MEMCPY_NAME __memcpy_unaligned
+#endif
+
+#define LD_64(reg, n) \
+	ld.d    t0, reg, n;    \
+	ld.d    t1, reg, n+8;  \
+	ld.d    t2, reg, n+16; \
+	ld.d    t3, reg, n+24; \
+	ld.d    t4, reg, n+32; \
+	ld.d    t5, reg, n+40; \
+	ld.d    t6, reg, n+48; \
+	ld.d    t7, reg, n+56;
+
+#define ST_64(reg, n) \
+	st.d    t0, reg, n;    \
+	st.d    t1, reg, n+8;  \
+	st.d    t2, reg, n+16; \
+	st.d    t3, reg, n+24; \
+	st.d    t4, reg, n+32; \
+	st.d    t5, reg, n+40; \
+	st.d    t6, reg, n+48; \
+	st.d    t7, reg, n+56;
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMCPY_NAME, 0)
+#else
+LEAF(MEMCPY_NAME)
+#endif
+
+//1st var: dst ptr: void *a1 $r4 a0
+//2nd var: src ptr: void *a2 $r5 a1
+//3rd var: size_t len $r6 a2
+//t0~t9 registers as temp
+
+	add.d   a4, a1, a2
+	add.d   a3, a0, a2
+	li.w    a6, 16
+	bge     a6, a2, less_16bytes
+	li.w    a6, 128
+	blt     a6, a2, long_bytes
+	li.w    a6, 64
+	blt     a6, a2, more_64bytes
+	li.w    a6, 32
+	blt     a6, a2, more_32bytes
+
+	/* 17...32 */
+	ld.d    t0, a1, 0
+	ld.d    t1, a1, 8
+	ld.d    t2, a4, -16
+	ld.d    t3, a4, -8
+	st.d    t0, a0, 0
+	st.d    t1, a0, 8
+	st.d    t2, a3, -16
+	st.d    t3, a3, -8
+	jr  ra
+
+more_64bytes:
+	srli.d	t8, a0, 3
+	slli.d	t8, t8, 3
+	addi.d	t8, t8,  0x8
+	sub.d	a7, a0, t8
+	ld.d	t0, a1, 0
+	sub.d	a1, a1, a7
+	st.d	t0, a0, 0
+
+	add.d	a7, a7, a2
+	addi.d	a7, a7, -0x20
+loop_32:
+	ld.d	t0, a1, 0
+	ld.d	t1, a1, 8
+	ld.d	t2, a1, 16
+	ld.d	t3, a1, 24
+	st.d	t0, t8, 0
+	st.d	t1, t8, 8
+	st.d	t2, t8, 16
+	st.d	t3, t8, 24
+
+	addi.d	t8,  t8,   0x20
+	addi.d	a1,  a1,   0x20
+	addi.d	a7,  a7,  -0x20
+	blt     zero, a7, loop_32
+
+	ld.d	t4, a4, -32
+	ld.d	t5, a4, -24
+	ld.d	t6, a4, -16
+	ld.d	t7, a4, -8
+	st.d	t4, a3, -32
+	st.d	t5, a3, -24
+	st.d	t6, a3, -16
+	st.d	t7, a3, -8
+
+	jr	ra
+
+more_32bytes:
+	/* 33...64 */
+	ld.d    t0, a1, 0
+	ld.d    t1, a1, 8
+	ld.d    t2, a1, 16
+	ld.d    t3, a1, 24
+	ld.d    t4, a4, -32
+	ld.d    t5, a4, -24
+	ld.d    t6, a4, -16
+	ld.d    t7, a4, -8
+	st.d    t0, a0, 0
+	st.d    t1, a0, 8
+	st.d    t2, a0, 16
+	st.d    t3, a0, 24
+	st.d    t4, a3, -32
+	st.d    t5, a3, -24
+	st.d    t6, a3, -16
+	st.d    t7, a3, -8
+	jr  ra
+
+less_16bytes:
+	srai.d  a6, a2, 3
+	beqz    a6, less_8bytes
+
+	/* 8...16 */
+	ld.d    t0, a1, 0
+	ld.d    t1, a4, -8
+	st.d    t0, a0, 0
+	st.d    t1, a3, -8
+
+	jr  ra
+
+less_8bytes:
+	srai.d  a6, a2, 2
+	beqz    a6, less_4bytes
+
+	/* 4...7 */
+	ld.w    t0, a1, 0
+	ld.w    t1, a4, -4
+	st.w    t0, a0, 0
+	st.w    t1, a3, -4
+	jr  ra
+
+less_4bytes:
+	srai.d  a6, a2, 1
+	beqz    a6, less_2bytes
+
+	/* 2...3 */
+	ld.h    t0, a1, 0
+	ld.h    t1, a4, -2
+	st.h    t0, a0, 0
+	st.h    t1, a3, -2
+	jr  ra
+
+less_2bytes:
+	beqz    a2, less_1bytes
+
+	ld.b    t0, a1, 0
+	st.b    t0, a0, 0
+	jr  ra
+
+less_1bytes:
+	jr  ra
+
+long_bytes:
+	srli.d  t8, a0, 3
+	slli.d  t8, t8, 3
+	beq     a0, t8, start
+
+	ld.d    t0, a1, 0
+	addi.d  t8, t8, 0x8
+	st.d    t0, a0, 0
+	sub.d   a7, a0, t8
+	sub.d   a1, a1, a7
+
+start:
+	addi.d  a5, a3, -0x80
+	blt     a5, t8, align_end_proc
+
+loop_128:
+	LD_64(a1, 0)
+	ST_64(t8, 0)
+	LD_64(a1, 64)
+	addi.d  a1, a1,  0x80
+	ST_64(t8, 64)
+	addi.d  t8, t8,  0x80
+	bge     a5, t8, loop_128
+
+align_end_proc:
+	sub.d   a2, a3, t8
+
+	pcaddi  t1, 34
+	andi    t2, a2, 0x78
+	sub.d   t1, t1, t2
+	jirl    zero, t1, 0
+
+end_120_128_unalign:
+	ld.d    t0, a1, 112
+	st.d    t0, t8, 112
+end_112_120_unalign:
+	ld.d    t0, a1, 104
+	st.d    t0, t8, 104
+end_104_112_unalign:
+	ld.d    t0, a1, 96
+	st.d    t0, t8, 96
+end_96_104_unalign:
+	ld.d    t0, a1, 88
+	st.d    t0, t8, 88
+end_88_96_unalign:
+	ld.d    t0, a1, 80
+	st.d    t0, t8, 80
+end_80_88_unalign:
+	ld.d    t0, a1, 72
+	st.d    t0, t8, 72
+end_72_80_unalign:
+	ld.d    t0, a1, 64
+	st.d    t0, t8, 64
+end_64_72_unalign:
+	ld.d    t0, a1, 56
+	st.d    t0, t8, 56
+end_56_64_unalign:
+	ld.d    t0, a1, 48
+	st.d    t0, t8, 48
+end_48_56_unalign:
+	ld.d    t0, a1, 40
+	st.d    t0, t8, 40
+end_40_48_unalign:
+	ld.d    t0, a1, 32
+	st.d    t0, t8, 32
+end_32_40_unalign:
+	ld.d    t0, a1, 24
+	st.d    t0, t8, 24
+end_24_32_unalign:
+	ld.d    t0, a1, 16
+	st.d    t0, t8, 16
+end_16_24_unalign:
+	ld.d    t0, a1, 8
+	st.d    t0, t8, 8
+end_8_16_unalign:
+	ld.d    t0, a1, 0
+	st.d    t0, t8, 0
+end_0_8_unalign:
+	ld.d    t0, a4, -8
+	st.d    t0, a3, -8
+
+	jr  ra
+
+END(MEMCPY_NAME)
+
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMCPY_NAME)
+#endif
+#endif
+
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memcpy.c b/sysdeps/loongarch/lp64/multiarch/memcpy.c
new file mode 100644
index 0000000000..c66858301d
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memcpy.c
@@ -0,0 +1,39 @@
+/* Multiple versions of memcpy.
+   All versions must be listed in ifunc-impl-list.c.
+   Copyright (C) 2017-2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <https://www.gnu.org/licenses/>.  */
+
+/* Define multiple versions only for the definition in libc.  */
+#if IS_IN (libc)
+# define memcpy __redirect_memcpy
+# include <string.h>
+# undef memcpy
+
+# define SYMBOL_NAME memcpy
+# include "ifunc-memcpy.h"
+
+libc_ifunc_redirected (__redirect_memcpy, __new_memcpy,
+		       IFUNC_SELECTOR ());
+
+# ifdef SHARED
+__hidden_ver1 (__new_memcpy, __GI_memcpy, __redirect_memcpy)
+  __attribute__ ((visibility ("hidden")));
+# endif
+
+# include <shlib-compat.h>
+versioned_symbol (libc, __new_memcpy, memcpy, GLIBC_2_27);
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memmove-unaligned.S b/sysdeps/loongarch/lp64/multiarch/memmove-unaligned.S
new file mode 100644
index 0000000000..d6db605d5a
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memmove-unaligned.S
@@ -0,0 +1,480 @@
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <regdef.h>
+#include <sys/asm.h>
+#endif
+
+#if IS_IN (libc)
+
+#ifndef MEMMOVE_NAME
+#define MEMMOVE_NAME __memmove_unaligned
+#endif
+
+#define LD_64(reg, n) \
+	ld.d	t0, reg, n;    \
+	ld.d	t1, reg, n+8;  \
+	ld.d	t2, reg, n+16; \
+	ld.d	t3, reg, n+24; \
+	ld.d	t4, reg, n+32; \
+	ld.d	t5, reg, n+40; \
+	ld.d	t6, reg, n+48; \
+	ld.d	t7, reg, n+56;
+
+
+#define ST_64(reg, n) \
+	st.d	t0, reg, n;    \
+	st.d	t1, reg, n+8;  \
+	st.d	t2, reg, n+16; \
+	st.d	t3, reg, n+24; \
+	st.d	t4, reg, n+32; \
+	st.d	t5, reg, n+40; \
+	st.d	t6, reg, n+48; \
+	st.d	t7, reg, n+56;
+
+#define LDST_1024 \
+	LD_64(a1, 0);    \
+	ST_64(a0, 0);    \
+	LD_64(a1, 64);   \
+	ST_64(a0, 64);   \
+	LD_64(a1, 128);  \
+	ST_64(a0, 128);  \
+	LD_64(a1, 192);  \
+	ST_64(a0, 192);  \
+	LD_64(a1, 256);  \
+	ST_64(a0, 256);  \
+	LD_64(a1, 320);  \
+	ST_64(a0, 320);  \
+	LD_64(a1, 384);  \
+	ST_64(a0, 384);  \
+	LD_64(a1, 448);  \
+	ST_64(a0, 448);  \
+	LD_64(a1, 512);  \
+	ST_64(a0, 512);  \
+	LD_64(a1, 576);  \
+	ST_64(a0, 576);  \
+	LD_64(a1, 640);  \
+	ST_64(a0, 640);  \
+	LD_64(a1, 704);  \
+	ST_64(a0, 704);  \
+	LD_64(a1, 768);  \
+	ST_64(a0, 768);  \
+	LD_64(a1, 832);  \
+	ST_64(a0, 832);  \
+	LD_64(a1, 896);  \
+	ST_64(a0, 896);  \
+	LD_64(a1, 960);  \
+	ST_64(a0, 960);
+
+#define LDST_1024_BACK \
+	LD_64(a4, -64);   \
+	ST_64(a3, -64);   \
+	LD_64(a4, -128);  \
+	ST_64(a3, -128);  \
+	LD_64(a4, -192);  \
+	ST_64(a3, -192);  \
+	LD_64(a4, -256);  \
+	ST_64(a3, -256);  \
+	LD_64(a4, -320);  \
+	ST_64(a3, -320);  \
+	LD_64(a4, -384);  \
+	ST_64(a3, -384);  \
+	LD_64(a4, -448);  \
+	ST_64(a3, -448);  \
+	LD_64(a4, -512);  \
+	ST_64(a3, -512);  \
+	LD_64(a4, -576);  \
+	ST_64(a3, -576);  \
+	LD_64(a4, -640);  \
+	ST_64(a3, -640);  \
+	LD_64(a4, -704);  \
+	ST_64(a3, -704);  \
+	LD_64(a4, -768);  \
+	ST_64(a3, -768);  \
+	LD_64(a4, -832);  \
+	ST_64(a3, -832);  \
+	LD_64(a4, -896);  \
+	ST_64(a3, -896);  \
+	LD_64(a4, -960);  \
+	ST_64(a3, -960);  \
+	LD_64(a4, -1024); \
+	ST_64(a3, -1024);
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMMOVE_NAME, 0)
+#else
+LEAF(MEMMOVE_NAME)
+#endif
+
+//1st var: dest ptr: void *str1 $r4 a0
+//2nd var: src  ptr: void *str2 $r5 a1
+//3rd var: size_t num
+//t0~t9 registers as temp
+
+	add.d	a4, a1, a2
+	add.d	a3, a0, a2
+	beq		a1, a0, less_1bytes
+	move	t8, a0
+	srai.d	a6, a2, 4  		#num/16
+	beqz	a6, less_16bytes        #num<16
+	srai.d	a6, a2, 6  		#num/64
+	bnez	a6, more_64bytes       #num>64
+	srai.d	a6, a2, 5
+	beqz	a6, less_32bytes	   #num<32
+
+	ld.d	t0, a1, 0              #32<num<64
+	ld.d	t1, a1, 8
+	ld.d	t2, a1, 16
+	ld.d	t3, a1, 24
+	ld.d	t4, a4, -32
+	ld.d	t5, a4, -24
+	ld.d	t6, a4, -16
+	ld.d	t7, a4, -8
+	st.d	t0, a0, 0
+	st.d	t1, a0, 8
+	st.d	t2, a0, 16
+	st.d	t3, a0, 24
+	st.d	t4, a3, -32
+	st.d	t5, a3, -24
+	st.d	t6, a3, -16
+	st.d	t7, a3, -8
+
+	jr  ra
+
+less_32bytes:
+	ld.d	t0, a1, 0
+	ld.d	t1, a1, 8
+	ld.d	t2, a4, -16
+	ld.d	t3, a4, -8
+	st.d	t0, a0, 0
+	st.d	t1, a0, 8
+	st.d	t2, a3, -16
+	st.d	t3, a3, -8
+
+	jr	ra
+
+less_16bytes:
+	srai.d	a6, a2, 3 #num/8
+	beqz	a6, less_8bytes
+
+	ld.d	t0, a1, 0
+	ld.d	t1, a4, -8
+	st.d	t0, a0, 0
+	st.d	t1, a3, -8
+
+	jr	ra
+
+less_8bytes:
+	srai.d	a6, a2, 2
+	beqz	a6, less_4bytes
+
+	ld.w	t0, a1, 0
+	ld.w	t1, a4, -4
+	st.w	t0, a0, 0
+	st.w	t1, a3, -4
+
+	jr	ra
+
+less_4bytes:
+	srai.d	a6, a2, 1
+	beqz	a6, less_2bytes
+
+	ld.h	t0, a1, 0
+	ld.h	t1, a4, -2
+	st.h	t0, a0, 0
+	st.h	t1, a3, -2
+
+	jr	ra
+
+less_2bytes:
+	beqz	a2, less_1bytes
+
+	ld.b	t0, a1, 0
+	st.b	t0, a0, 0
+
+	jr	ra
+
+less_1bytes:
+	jr	ra
+
+more_64bytes:
+	sub.d   a7, a0, a1
+	bltu	a7, a2, copy_backward
+
+copy_forward:
+	srli.d	a0, a0, 3
+	slli.d	a0, a0, 3
+	beq 	a0, t8, all_align
+	addi.d	a0, a0, 0x8
+	sub.d	a7, t8, a0
+	sub.d	a1, a1, a7
+	add.d	a2, a7, a2
+
+start_unalign_proc:
+	pcaddi  t1, 18
+	slli.d  a6, a7, 3
+	add.d   t1, t1, a6
+	jirl    zero, t1, 0
+
+start_7_unalign:
+	ld.b    t0, a1, -7
+	st.b    t0, a0, -7
+start_6_unalign:
+	ld.b    t0, a1, -6
+	st.b    t0, a0, -6
+start_5_unalign:
+	ld.b    t0, a1, -5
+	st.b    t0, a0, -5
+start_4_unalign:
+	ld.b    t0, a1, -4
+	st.b    t0, a0, -4
+start_3_unalign:
+	ld.b    t0, a1, -3
+	st.b    t0, a0, -3
+start_2_unalign:
+	ld.b    t0, a1, -2
+	st.b    t0, a0, -2
+start_1_unalign:
+	ld.b    t0, a1, -1
+	st.b    t0, a0, -1
+start_over:
+
+	addi.d	a2, a2, -0x80
+	blt     a2, zero, end_unalign_proc
+
+loop_less:
+	LD_64(a1, 0)
+	ST_64(a0, 0)
+	LD_64(a1, 64)
+	ST_64(a0, 64)
+
+	addi.d	a0, a0,  0x80
+	addi.d	a1, a1,  0x80
+	addi.d	a2, a2, -0x80
+	bge     a2, zero, loop_less
+
+end_unalign_proc:
+	addi.d  a2, a2, 0x80
+
+    	pcaddi  t1, 36
+    	andi    t2, a2, 0x78
+	add.d   a1, a1, t2
+	add.d   a0, a0, t2
+    	sub.d   t1, t1, t2
+    	jirl    zero, t1, 0
+
+end_120_128_unalign:
+	ld.d    t0, a1, -120
+	st.d    t0, a0, -120
+end_112_120_unalign:
+	ld.d    t0, a1, -112
+	st.d    t0, a0, -112
+end_104_112_unalign:
+	ld.d    t0, a1, -104
+	st.d    t0, a0, -104
+end_96_104_unalign:
+	ld.d    t0, a1, -96
+	st.d    t0, a0, -96
+end_88_96_unalign:
+	ld.d    t0, a1, -88
+	st.d    t0, a0, -88
+end_80_88_unalign:
+	ld.d    t0, a1, -80
+	st.d    t0, a0, -80
+end_72_80_unalign:
+	ld.d    t0, a1, -72
+	st.d    t0, a0, -72
+end_64_72_unalign:
+	ld.d    t0, a1, -64
+	st.d    t0, a0, -64
+end_56_64_unalign:
+	ld.d    t0, a1, -56
+	st.d    t0, a0, -56
+end_48_56_unalign:
+	ld.d    t0, a1, -48
+	st.d    t0, a0, -48
+end_40_48_unalign:
+	ld.d    t0, a1, -40
+	st.d    t0, a0, -40
+end_32_40_unalign:
+	ld.d    t0, a1, -32
+	st.d    t0, a0, -32
+end_24_32_unalign:
+    	ld.d    t0, a1, -24
+    	st.d    t0, a0, -24
+end_16_24_unalign:
+    	ld.d    t0, a1, -16
+    	st.d    t0, a0, -16
+end_8_16_unalign:
+    	ld.d    t0, a1, -8
+    	st.d    t0, a0, -8
+end_0_8_unalign:
+
+    	andi    a2, a2, 0x7
+	pcaddi  t1, 18
+	slli.d  a2, a2, 3
+	sub.d   t1, t1, a2
+	jirl    zero, t1, 0
+
+end_7_unalign:
+	ld.b    t0, a4, -7
+	st.b    t0, a3, -7
+end_6_unalign:
+	ld.b    t0, a4, -6
+	st.b    t0, a3, -6
+end_5_unalign:
+	ld.b    t0, a4, -5
+	st.b    t0, a3, -5
+end_4_unalign:
+	ld.b    t0, a4, -4
+	st.b    t0, a3, -4
+end_3_unalign:
+	ld.b    t0, a4, -3
+	st.b    t0, a3, -3
+end_2_unalign:
+	ld.b    t0, a4, -2
+	st.b    t0, a3, -2
+end_1_unalign:
+	ld.b    t0, a4, -1
+	st.b    t0, a3, -1
+end:
+
+	move    v0, t8
+	jr	ra
+
+all_align:
+	addi.d  a1, a1, 0x8
+	addi.d  a0, a0, 0x8
+	ld.d	t0, a1, -8
+	st.d    t0, a0, -8
+	addi.d  a2, a2, -8
+	b 		start_over
+
+all_align_back:
+	addi.d  a4, a4, -0x8
+	addi.d  a3, a3, -0x8
+	ld.d    t0, a4, 0
+	st.d    t0, a3, 0
+	addi.d  a2, a2, -8
+	b       start_over_back
+
+copy_backward:
+	move    a5, a3
+	srli.d  a3, a3, 3
+	slli.d  a3, a3, 3
+	beq     a3, a5, all_align_back
+	sub.d   a7, a3, a5
+	add.d   a4, a4, a7
+	add.d   a2, a7, a2
+
+	pcaddi  t1, 18
+	slli.d  a6, a7, 3
+	add.d   t1, t1, a6
+	jirl    zero, t1, 0
+
+	ld.b    t0, a4, 6
+	st.b    t0, a3, 6
+	ld.b    t0, a4, 5
+	st.b    t0, a3, 5
+	ld.b    t0, a4, 4
+	st.b    t0, a3, 4
+	ld.b    t0, a4, 3
+	st.b    t0, a3, 3
+	ld.b    t0, a4, 2
+	st.b    t0, a3, 2
+	ld.b    t0, a4, 1
+	st.b    t0, a3, 1
+	ld.b    t0, a4, 0
+	st.b    t0, a3, 0
+start_over_back:
+
+	addi.d  a2, a2, -0x80
+	blt     a2, zero, end_unalign_proc_back
+
+loop_less_back:
+	LD_64(a4, -64)
+	ST_64(a3, -64)
+	LD_64(a4, -128)
+	ST_64(a3, -128)
+
+	addi.d a4, a4, -0x80
+	addi.d a3, a3, -0x80
+	addi.d a2, a2, -0x80
+	bge    a2, zero, loop_less_back
+
+end_unalign_proc_back:
+	addi.d  a2, a2, 0x80
+
+	pcaddi  t1, 36
+	andi    t2, a2, 0x78
+	sub.d   a4, a4, t2
+	sub.d   a3, a3, t2
+	sub.d   t1, t1, t2
+	jirl    zero, t1, 0
+
+	ld.d    t0, a4, 112
+	st.d    t0, a3, 112
+	ld.d    t0, a4, 104
+	st.d    t0, a3, 104
+	ld.d    t0, a4, 96
+	st.d    t0, a3, 96
+	ld.d    t0, a4, 88
+	st.d    t0, a3, 88
+	ld.d    t0, a4, 80
+	st.d    t0, a3, 80
+	ld.d    t0, a4, 72
+	st.d    t0, a3, 72
+	ld.d    t0, a4, 64
+	st.d    t0, a3, 64
+	ld.d    t0, a4, 56
+	st.d    t0, a3, 56
+	ld.d    t0, a4, 48
+	st.d    t0, a3, 48
+	ld.d    t0, a4, 40
+	st.d    t0, a3, 40
+	ld.d    t0, a4, 32
+	st.d    t0, a3, 32
+    	ld.d    t0, a4, 24
+    	st.d    t0, a3, 24
+    	ld.d    t0, a4, 16
+    	st.d    t0, a3, 16
+    	ld.d    t0, a4, 8
+    	st.d    t0, a3, 8
+	ld.d    t0, a4, 0
+	st.d    t0, a3, 0
+
+	andi    a2, a2, 0x7
+	pcaddi  t1, 18
+	slli.d  a2, a2, 3
+	sub.d   t1, t1, a2
+	jirl    zero, t1, 0
+
+	ld.b    t0, a1, 6
+	st.b    t0, a0, 6
+	ld.b    t0, a1, 5
+	st.b    t0, a0, 5
+	ld.b    t0, a1, 4
+	st.b    t0, a0, 4
+	ld.b    t0, a1, 3
+	st.b    t0, a0, 3
+	ld.b    t0, a1, 2
+	st.b    t0, a0, 2
+	ld.b    t0, a1, 1
+	st.b    t0, a0, 1
+	ld.b    t0, a1, 0
+	st.b    t0, a0, 0
+
+	move    v0, t8
+	jr	ra
+
+END(MEMMOVE_NAME)
+
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMMOVE_NAME)
+#endif
+#endif
+
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memmove.c b/sysdeps/loongarch/lp64/multiarch/memmove.c
new file mode 100644
index 0000000000..a877640465
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memmove.c
@@ -0,0 +1,39 @@
+/* Multiple versions of memcpy.
+   All versions must be listed in ifunc-impl-list.c.
+   Copyright (C) 2017-2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <https://www.gnu.org/licenses/>.  */
+
+/* Define multiple versions only for the definition in libc.  */
+#if IS_IN (libc)
+# define memmove __redirect_memmove
+# include <string.h>
+# undef memmove
+
+# define SYMBOL_NAME memmove
+# include "ifunc-memmove.h"
+
+libc_ifunc_redirected (__redirect_memmove, __new_memmove,
+		       IFUNC_SELECTOR ());
+
+# ifdef SHARED
+__hidden_ver1 (__new_memmove, __GI_memmove, __redirect_memmove)
+  __attribute__ ((visibility ("hidden")));
+# endif
+
+# include <shlib-compat.h>
+versioned_symbol (libc, __new_memmove, memmove, GLIBC_2_27);
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memset-aligned.S b/sysdeps/loongarch/lp64/multiarch/memset-aligned.S
new file mode 100644
index 0000000000..169692c078
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memset-aligned.S
@@ -0,0 +1,11 @@
+
+#if IS_IN (libc)
+
+#ifndef MEMSET_NAME
+#define MEMSET_NAME __memset_aligned
+#endif
+
+#endif
+
+#include "../memset.S"
+
diff --git a/sysdeps/loongarch/lp64/multiarch/memset-lasx.S b/sysdeps/loongarch/lp64/multiarch/memset-lasx.S
new file mode 100644
index 0000000000..5c6522d815
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memset-lasx.S
@@ -0,0 +1,330 @@
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <sys/asm.h>
+#include <sys/regdef.h>
+#endif
+
+#define XVXOR_V(xd,xj,xk)       .word(0x1d<<26|0x09<<21|0x0E<<15|(xk&0x1f)<<10|(xj&0x1f)<<5|(xd&0x1f))
+#define XVREPLVE0_B(xd,xj)      .word(0x1d<<26|0x18<<21|0x0E<<15|0x0<<10|(xj&0x1f)<<5|(xd&0x1f))
+#define XVST(xd,rj,si12)        .word(0x0b<<26|0x3<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(xd&0x1f))
+#define VST(vd,rj,si12)         .word(0x0b<<26|0x1<<22|(si12&0xfff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_D(vd,rj,si8,idx) .word(0x31<<24|0x2<<19|(idx&0x1)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_W(vd,rj,si8,idx) .word(0x31<<24|0x2<<20|(idx&0x3)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_H(vd,rj,si8,idx) .word(0x31<<24|0x2<<21|(idx&0x7)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+#define VSTELM_B(vd,rj,si8,idx) .word(0x31<<24|0x2<<22|(idx&0xf)<<18|(si8&0xff)<<10|(rj&0x1f)<<5|(vd&0x1f))
+
+//offset(10b) -> si12
+
+//1st var: void *str  $4 a0
+//2nd var: int val  $5   a1
+//3rd var: size_t num  $6  a2
+
+#if IS_IN (libc)
+
+/* Allow the routine to be named something else if desired.  */
+#ifndef MEMSET_NAME
+#define MEMSET_NAME __memset_lasx
+#endif
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMSET_NAME, 0)
+#else
+LEAF(MEMSET_NAME)
+#endif
+
+	.align	6
+	XVXOR_V(0,0,0)
+	XVXOR_V(1,1,1)
+  	#dmtc1	a1, $f1  # 32bit, 2nd var; or use FILL.d inst  ###use FILL.d inst, replacing dmtc1 inst probably needs multiple insts
+  	#FILL.D $w0, a1
+  	#XVREPLGR2VR.B $w1, a1
+  	.word(0x1da7c0<<10|(0x5&0x1f)<<5|(0x1&0x1f))
+
+	add.d	t7, a0, a2 # dest, 1st var, 3rd var
+	move	t0, a0      # $2, func return value  "v0->a0 v1->a1" need another register to store original parameters
+	XVREPLVE0_B(0,1)  # w0 <- w1
+	srai.d	t8, a2, 4  #num/16
+	beqz	t8, less_16bytes       # num<16
+	nop
+	srai.d	t8, a2, 8  #num/256
+	bnez	t8, eqormore_256bytes  #num>256
+	nop
+	srai.d	t8, a2, 7
+	beqz	t8, less_128bytes	#num<128_
+	nop
+	XVST( 0, 4, 0 )  # 128<num<256
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 19, -128 )
+	XVST( 0, 19, -96 )
+	XVST( 0, 19, -64 )
+	XVST( 0, 19, -32 )
+
+	jr	ra
+	nop
+
+less_128bytes:
+	srai.d	t8, a2, 6  #num/64
+	beqz	t8, less_64bytes
+	nop
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 19, -64 )
+	XVST( 0, 19, -32 )
+
+	jr	ra
+	nop
+
+less_64bytes:
+	srai.d	t8, a2, 5 #num/32
+	beqz	t8, less_32bytes
+	nop
+	XVST( 0, 4, 0 )
+	XVST( 0, 19, -32 )
+
+	jr	ra
+	nop
+
+less_32bytes:
+	VST( 0, 4, 0 )
+	VST( 0, 19, -16 )
+	jr	ra
+	nop
+
+less_16bytes:
+	srai.d	t8, a2, 3 #num/8
+	beqz	t8, less_8bytes
+	nop
+	//sdc1	$f0, 0(a0)	#store lower 8bytes to mem
+	//sdc1	$f0, -8($15)	#store lower 8bytes to mem
+	VSTELM_D (0, 4,  0,  0)      #store lower 8bytes to mem
+	VSTELM_D (0, 19, -1, 0)	#store lower 8bytes to mem
+	jr	ra
+	nop
+
+less_8bytes:
+	srai.d	t8, a2, 2
+	beqz	t8, less_4bytes
+	nop
+	//swc1	$f0, 0(a0)	#store lower 4bytes to mem
+	//swc1	$f0, -4($15)	#store lower 4bytes to mem
+	VSTELM_W (0, 4,  0,  0)      #store lower 4bytes to mem
+	VSTELM_W (0, 19, -1, 0)	#store lower 4bytes to mem
+	jr	ra
+	nop
+
+less_4bytes:
+	//mfc1	$25, $f0   #uesless
+	srai.d	t8, a2, 1
+	beqz	t8, less_2bytes
+	nop
+	//sh	$25, 0(a0)	#store lower 2bytes to mem
+	//sh	$25, -2($15)	#store lower 2bytes to mem
+	VSTELM_H (0, 4,  0,  0)      #store lower 2bytes to mem
+	VSTELM_H (0, 19, -1, 0)	#store lower 2bytes to mem
+	jr	ra
+	nop
+
+less_2bytes:
+	//beqz	t8, less_1bytes
+	beqz	a2, less_1bytes
+	nop
+	//sb	$25, 0(a0)	#store lower 1bytes to mem
+	VSTELM_B (0, 4,  0,  0)      #store lower 1bytes to mem
+	jr	ra
+	nop
+
+less_1bytes:
+	jr	ra
+	nop
+
+
+eqormore_256bytes:
+	// andi	a0, a0, -0x20
+	srli.d	a0, a0, 5
+	slli.d	a0, a0, 5     # a0:
+	addi.d	a0, a0,  0x20  #align to 32   no implememt for daddi
+	XVST( 0, 12, 0 )    # unaligned data
+	sub.d	t2, t0,  a0    # $2:start addr a0 > t0   t0-a0 < 0  no operation for overflow
+	add.d	t2, t2, a2     # a2:num
+	addi.d	t2, t2, -0x80    # used in loop_less
+	srai.d	t8, t2, 12     #t2/4096
+	beqz	t8, loop_less
+	nop
+
+loop_more:                   # t8 >0
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512   #1
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #2
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #3
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #4
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #5
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #6
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #7
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	XVST( 0, 4, 128 )
+	XVST( 0, 4, 160 )
+	XVST( 0, 4, 192 )
+	XVST( 0, 4, 224 )
+	XVST( 0, 4, 256 )
+	XVST( 0, 4, 288 )
+	XVST( 0, 4, 320 )
+	XVST( 0, 4, 352 )
+	XVST( 0, 4, 384 )
+	XVST( 0, 4, 416 )
+	XVST( 0, 4, 448 )
+	XVST( 0, 4, 480 )
+	addi.d	a0,  a0,   512  #8
+	addi.d	t8, t8, -1
+	bnez	t8, loop_more
+	nop
+  #andi	t2, t2, 4095   replaced with two insts
+  lu12i.w t3, 1        #get imm 1<<12
+  addi.d t3, t3, -1
+  and t2, t2, t3
+
+loop_less:			# t8 = 0
+	XVST( 0, 4, 0 )
+	XVST( 0, 4, 32 )
+	XVST( 0, 4, 64 )
+	XVST( 0, 4, 96 )
+	addi.d	a0,  a0,   0x80
+	addi.d	t2, t2, -0x80
+	slt	t8, t2, zero
+	beqz	t8, loop_less
+	nop
+	XVST( 0, 19, -128 )
+	XVST( 0, 19, -96 )
+	XVST( 0, 19, -64 )
+	XVST( 0, 19, -32 )
+
+  move	v0, t0  #change a0 only in eqormore_256bytes fragmemt, need restore from t0
+	jr	ra
+	nop
+END(MEMSET_NAME)
+
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMSET_NAME)
+#endif
+#endif
+
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memset-unaligned.S b/sysdeps/loongarch/lp64/multiarch/memset-unaligned.S
new file mode 100644
index 0000000000..64aa251d28
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memset-unaligned.S
@@ -0,0 +1,179 @@
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <sys/asm.h>
+#include <sys/regdef.h>
+#endif
+
+#if IS_IN (libc)
+
+#ifndef MEMSET_NAME
+#define MEMSET_NAME __memset_unaligned
+#endif
+
+#define ST_128(n) 	\
+	st.d	a1, a0, n;		 \
+	st.d    a1, a0, n+8  ; 	 \
+	st.d    a1, a0, n+16 ;   \
+	st.d    a1, a0, n+24 ;   \
+	st.d    a1, a0, n+32 ;   \
+	st.d    a1, a0, n+40 ;   \
+	st.d    a1, a0, n+48 ;   \
+	st.d    a1, a0, n+56 ;   \
+	st.d    a1, a0, n+64 ;   \
+	st.d    a1, a0, n+72 ;   \
+	st.d    a1, a0, n+80 ;   \
+	st.d    a1, a0, n+88 ;   \
+	st.d    a1, a0, n+96 ;   \
+	st.d    a1, a0, n+104;   \
+	st.d    a1, a0, n+112;   \
+	st.d    a1, a0, n+120;	 \
+
+//1st var: void *str  $4 a0
+//2nd var: int val  $5   a1
+//3rd var: size_t num  $6  a2
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMSET_NAME, 0)
+#else
+LEAF(MEMSET_NAME)
+#endif
+
+	.align	6
+	bstrins.d a1, a1, 15, 8
+	add.d	  t7, a0, a2
+	bstrins.d a1, a1, 31, 16
+	move	  t0, a0
+	bstrins.d a1, a1, 63, 32
+	srai.d	  t8, a2, 4         	#num/16
+	beqz	  t8, less_16bytes	#num<16
+	srai.d	  t8, a2, 6		#num/64
+	bnez	  t8, more_64bytes	#num>64
+	srai.d	  t8, a2, 5		#num/32
+	beqz	  t8, less_32bytes	#num<32
+	st.d	  a1, a0, 0 		#32<num<64
+	st.d	  a1, a0, 8
+	st.d	  a1, a0, 16
+	st.d	  a1, a0, 24
+	st.d	  a1, t7, -32
+	st.d	  a1, t7, -24
+	st.d	  a1, t7, -16
+	st.d	  a1, t7, -8
+
+	jr	  ra
+
+less_32bytes:
+	st.d	  a1, a0, 0
+	st.d	  a1, a0, 8
+	st.d	  a1, t7, -16
+	st.d	  a1, t7, -8
+
+	jr	  ra
+
+less_16bytes:
+	srai.d	  t8, a2, 3		#num/8
+	beqz	  t8, less_8bytes
+	st.d	  a1, a0, 0
+	st.d	  a1, t7, -8
+
+	jr	  ra
+
+less_8bytes:
+	srai.d	  t8, a2, 2
+	beqz	  t8, less_4bytes
+	st.w	  a1, a0, 0
+	st.w	  a1, t7, -4
+
+	jr	  ra
+
+less_4bytes:
+	srai.d	  t8, a2, 1
+	beqz	  t8, less_2bytes
+	st.h	  a1, a0, 0
+	st.h	  a1, t7, -2
+
+	jr	  ra
+
+less_2bytes:
+	beqz	  a2, less_1bytes
+	st.b	  a1, a0, 0
+
+	jr	  ra
+
+less_1bytes:
+	jr	  ra
+
+more_64bytes:
+	srli.d	  a0, a0, 3
+	slli.d	  a0, a0, 3
+	addi.d	  a0, a0, 0x8
+	st.d      a1, t0, 0
+	sub.d	  t2, t0, a0
+	add.d	  a2, t2, a2
+
+	addi.d	  a2, a2, -0x80
+	blt       a2, zero, end_unalign_proc
+
+loop_less:
+	ST_128(0)
+	addi.d	a0, a0,  0x80
+	addi.d	a2, a2, -0x80
+	bge     a2, zero, loop_less
+
+end_unalign_proc:
+	addi.d  a2, a2, 0x80
+
+	pcaddi  t1, 20
+	andi    t5, a2, 0x78
+	srli.d  t5, t5, 1
+	sub.d   t1, t1, t5
+	jirl    zero, t1, 0
+
+end_120_128_unalign:
+	st.d    a1, a0, 112
+end_112_120_unalign:
+	st.d    a1, a0, 104
+end_104_112_unalign:
+	st.d    a1, a0, 96
+end_96_104_unalign:
+	st.d    a1, a0, 88
+end_88_96_unalign:
+	st.d    a1, a0, 80
+end_80_88_unalign:
+	st.d    a1, a0, 72
+end_72_80_unalign:
+	st.d    a1, a0, 64
+end_64_72_unalign:
+	st.d    a1, a0, 56
+end_56_64_unalign:
+	st.d    a1, a0, 48
+end_48_56_unalign:
+	st.d    a1, a0, 40
+end_40_48_unalign:
+	st.d    a1, a0, 32
+end_32_40_unalign:
+	st.d    a1, a0, 24
+end_24_32_unalign:
+    st.d    a1, a0, 16
+end_16_24_unalign:
+    st.d    a1, a0, 8
+end_8_16_unalign:
+    st.d    a1, a0, 0
+end_0_8_unalign:
+
+	st.d    a1, t7, -8
+
+	move	  v0, t0
+	jr	  ra
+
+END(MEMSET_NAME)
+
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMSET_NAME)
+#endif
+#endif
+
+#endif
diff --git a/sysdeps/loongarch/lp64/multiarch/memset.c b/sysdeps/loongarch/lp64/multiarch/memset.c
new file mode 100644
index 0000000000..951f2b0bb1
--- /dev/null
+++ b/sysdeps/loongarch/lp64/multiarch/memset.c
@@ -0,0 +1,39 @@
+/* Multiple versions of memset.
+   All versions must be listed in ifunc-impl-list.c.
+   Copyright (C) 2017-2022 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, see
+   <https://www.gnu.org/licenses/>.  */
+
+/* Define multiple versions only for the definition in libc.  */
+#if IS_IN (libc)
+# define memset __redirect_memset
+# include <string.h>
+# undef memset
+
+# define SYMBOL_NAME memset
+# include "ifunc-memcpy.h"
+
+libc_ifunc_redirected (__redirect_memset, __new_memset,
+		       IFUNC_SELECTOR ());
+
+# ifdef SHARED
+__hidden_ver1 (__new_memset, __GI_memset, __redirect_memset)
+  __attribute__ ((visibility ("hidden")));
+# endif
+
+# include <shlib-compat.h>
+versioned_symbol (libc, __new_memset, memset, GLIBC_2_27);
+#endif
-- 
2.20.1

