From 466f2dbb96513c9f88a2c6fd69415985ceb6c096 Mon Sep 17 00:00:00 2001
From: caiyinyu <caiyinyu@loongson.cn>
Date: Tue, 11 Apr 2023 19:33:13 +0800
Subject: [PATCH 1/2] glibc-2.28: use latest memmove-lasx.S

Change-Id: I47bd36db5f552c02ac27f305eee8e619e18e3eb6
---
 .../loongarch/lp64/multiarch/memmove-lasx.S   | 285 ++++++++----------
 1 file changed, 132 insertions(+), 153 deletions(-)

diff --git a/sysdeps/loongarch/lp64/multiarch/memmove-lasx.S b/sysdeps/loongarch/lp64/multiarch/memmove-lasx.S
index bba2286776..9537a35a23 100644
--- a/sysdeps/loongarch/lp64/multiarch/memmove-lasx.S
+++ b/sysdeps/loongarch/lp64/multiarch/memmove-lasx.S
@@ -19,144 +19,123 @@
 
 LEAF(MEMCPY_NAME)
     .align          6
-    li.d            t6, 32
+
+    li.d            t0, 32
     add.d           a3, a0, a2
     add.d           a4, a1, a2
-    bgeu            t6, a2, L(less_32bytes) # a2 <= 32
-
-    li.d            t8, 128
-    li.d            t7, 64
-    bltu            t8, a2, L(copy_long)    # a2 > 128
-    bltu            t7, a2, L(more_64bytes) # a2 > 64
+    bgeu            t0, a2, L(less_32bytes) # a2 <= 32
 
+    li.d            t1, 64
+    bltu            t1, a2, L(copy_long)    # a2 > 64
     xvld            $xr0, a1, 0
     xvld            $xr1, a4, -32
-    xvst            $xr0, a0, 0
-    xvst            $xr1, a3, -32
-
-    jr              ra
-L(more_64bytes):
-    xvld            $xr0, a1, 0
-    xvld            $xr1, a1, 32
-    xvld            $xr2, a4, -64
 
-
-    xvld            $xr3, a4, -32
     xvst            $xr0, a0, 0
-    xvst            $xr1, a0, 32
-    xvst            $xr2, a3, -64
-
-    xvst            $xr3, a3, -32
+    xvst            $xr1, a3, -32
     jr              ra
 L(less_32bytes):
     srli.d          t0, a2, 4
-    beqz            t0, L(less_16bytes)
 
+    beqz            t0, L(less_16bytes)
     vld             $vr0, a1, 0
     vld             $vr1, a4, -16
     vst             $vr0, a0, 0
-    vst             $vr1, a3, -16
 
+
+    vst             $vr1, a3, -16
     jr              ra
 L(less_16bytes):
     srli.d          t0, a2, 3
     beqz            t0, L(less_8bytes)
-    vldrepl.d       $vr0, a1, 0
 
+    ld.d            t0, a1, 0
+    ld.d            t1, a4, -8
+    st.d            t0, a0, 0
+    st.d            t1, a3, -8
 
-    vldrepl.d       $vr1, a4, -8
-    vstelm.d        $vr0, a0, 0, 0
-    vstelm.d        $vr1, a3, -8, 0
     jr              ra
-
 L(less_8bytes):
     srli.d          t0, a2, 2
     beqz            t0, L(less_4bytes)
-    vldrepl.w       $vr0, a1, 0
-    vldrepl.w       $vr1, a4, -4
+    ld.w            t0, a1, 0
 
-    vstelm.w        $vr0, a0, 0, 0
-    vstelm.w        $vr1, a3, -4, 0
+    ld.w            t1, a4, -4
+    st.w            t0, a0, 0
+    st.w            t1, a3, -4
     jr              ra
+
+
 L(less_4bytes):
     srli.d          t0, a2, 1
-
     beqz            t0, L(less_2bytes)
-    vldrepl.h       $vr0, a1, 0
-    vldrepl.h       $vr1, a4, -2
-    vstelm.h        $vr0, a0, 0, 0
-
+    ld.h            t0, a1, 0
+    ld.h            t1, a4, -2
 
-    vstelm.h        $vr1, a3, -2, 0
+    st.h            t0, a0, 0
+    st.h            t1, a3, -2
     jr              ra
 L(less_2bytes):
     beqz            a2, L(less_1bytes)
-    ld.b            t0, a1, 0
 
+    ld.b            t0, a1, 0
     st.b            t0, a0, 0
 L(less_1bytes):
     jr              ra
-    nop
-    nop
 END(MEMCPY_NAME)
 
 LEAF(MEMMOVE_NAME)
-    li.d            t6, 32
+    .align          6
+
+    li.d            t0, 32
     add.d           a3, a0, a2
     add.d           a4, a1, a2
-    bgeu            t6, a2, L(less_32bytes) # a2 <= 32
-
-    li.d            t8, 128
-    li.d            t7, 64
-    bltu            t8, a2, L(move_long)    # a2 > 128
-    bltu            t7, a2, L(more_64bytes) # a2 > 64
-
+    bgeu            t0, a2, L(less_32bytes) # a2 <= 32
 
+    li.d            t1, 64
+    bltu            t1, a2, L(move_long)    # a2 > 64
     xvld            $xr0, a1, 0
     xvld            $xr1, a4, -32
+
     xvst            $xr0, a0, 0
     xvst            $xr1, a3, -32
-
     jr              ra
-    nop
 L(move_long):
-    sub.d           t0, a0, a1
-    bltu            t0, a2, L(copy_back)
+    sub.d           t2, a0, a1
 
+    bltu            t2, a2, L(copy_back)
 L(copy_long):
-    xvld            $xr1, a1, 0
-    andi            t0, a0, 0x1f
-    sub.d           t0, t6, t0
-    add.d           a1, a1, t0
+    andi            t2, a0, 0x1f
+    addi.d          a2, a2, -1
+    sub.d           t2, t0, t2
 
-    sub.d           a2, a2, t0
-    xvld            $xr0, a1, 0
-    addi.d          t1, a2, -32
-    add.d           a5, a0, t0
 
+    xvld            $xr8, a1, 0
+    xvld            $xr9, a4, -32
+    sub.d           t3, a2, t2
+    add.d           a5, a0, t2
 
-    andi            a2, t1, 0xff
-    xvst            $xr1, a0, 0
-    beq             t1, a2, L(long_end)
-    sub.d           t0, t1, a2
+    andi            a2, t3, 0xff
+    add.d           a1, a1, t2
+    beq             a2, t3, L(lt256)
+    sub.d           a6, a4, a2
 
-    add.d           a6, a1, t0
+    addi.d          a6, a6, -1
 L(loop_256):
+    xvld            $xr0, a1, 0
     xvld            $xr1, a1, 32
     xvld            $xr2, a1, 64
-    xvld            $xr3, a1, 96
 
+    xvld            $xr3, a1, 96
     xvld            $xr4, a1, 128
     xvld            $xr5, a1, 160
     xvld            $xr6, a1, 192
-    xvld            $xr7, a1, 224
 
-    xvst            $xr0, a5, 0
-    xvld            $xr0, a1, 256
+
+    xvld            $xr7, a1, 224
     addi.d          a1, a1, 256
+    xvst            $xr0, a5, 0
     xvst            $xr1, a5, 32
 
-
     xvst            $xr2, a5, 64
     xvst            $xr3, a5, 96
     xvst            $xr4, a5, 128
@@ -167,128 +146,128 @@ L(loop_256):
     addi.d          a5, a5, 256
     bne             a1, a6, L(loop_256)
 
-L(long_end):
-    bltu            a2, t8, L(end_less_128)
+L(lt256):
+    srli.d          t2, a2, 7
+    beqz            t2, L(lt128)
+    xvld            $xr0, a1, 0
     xvld            $xr1, a1, 32
+
+
     xvld            $xr2, a1, 64
     xvld            $xr3, a1, 96
-
-    addi.d          a2, a2, -128
-    xvst            $xr0, a5, 0
-    xvld            $xr0, a1, 128
     addi.d          a1, a1, 128
+    addi.d          a2, a2, -128
 
-
+    xvst            $xr0, a5, 0
     xvst            $xr1, a5, 32
     xvst            $xr2, a5, 64
     xvst            $xr3, a5, 96
-    addi.d          a5, a5, 128
 
-L(end_less_128):
-    blt             a2, t7, L(end_less_64)
+    addi.d          a5, a5, 128
+L(lt128):
+    bltu            a2, t1, L(lt64)
+    xvld            $xr0, a1, 0
     xvld            $xr1, a1, 32
-    addi.d          a2, a2, -64
-    xvst            $xr0, a5, 0
 
-    xvld            $xr0, a1, 64
     addi.d          a1, a1, 64
-    xvst            $xr1, a5, 32
-    addi.d          a5, a5, 64
-
-L(end_less_64):
-    blt             a2, t6, L(end_less_32)
+    addi.d          a2, a2, -64
     xvst            $xr0, a5, 0
-    xvld            $xr0, a1, 32
-    addi.d          a5, a5, 32
+    xvst            $xr1, a5, 32
 
 
-L(end_less_32):
-    xvld            $xr1, a4, -32
+    addi.d          a5, a5, 64
+L(lt64):
+    bltu            a2, t0, L(lt32)
+    xvld            $xr0, a1, 0
     xvst            $xr0, a5, 0
-    xvst            $xr1, a3, -32
+
+L(lt32):
+    xvst            $xr8, a0, 0
+    xvst            $xr9, a3, -32
     jr              ra
+    nop
 
 L(copy_back):
-    xvld            $xr1, a4, -32
-    andi            t0, a3, 0x1f
-    sub.d           a4, a4, t0
-    sub.d           a2, a2, t0
+    addi.d          a3, a3, -1
+    addi.d          a2, a2, -2
+    andi            t2, a3, 0x1f
+    xvld            $xr8, a1, 0
 
-    xvld            $xr0, a4, -32
-    addi.d          t1, a2, -32
-    xvst            $xr1, a3, -32
-    sub.d           a3, a3, t0
+    xvld            $xr9, a4, -32
+    sub.d           t3, a2, t2
+    sub.d           a5, a3, t2
+    sub.d           a4, a4, t2
 
-    andi            a2, t1, 0xff
-    beq             t1, a2, L(back_long_end)
-    sub.d           t1, t1, a2
-    sub.d           a6, a4, t1
 
+    andi            a2, t3, 0xff
+    beq             a2, t3, L(back_lt256)
+    add.d           a6, a1, a2
+    addi.d          a6, a6, 2
 
 L(back_loop_256):
-    xvld            $xr1, a4, -64
-    xvld            $xr2, a4, -96
-    xvld            $xr3, a4, -128
-    xvld            $xr4, a4, -160
+    xvld            $xr0, a4, -33
+    xvld            $xr1, a4, -65
+    xvld            $xr2, a4, -97
+    xvld            $xr3, a4, -129
 
-    xvld            $xr5, a4, -192
-    xvld            $xr6, a4, -224
-    xvld            $xr7, a4, -256
-    xvst            $xr0, a3, -32
+    xvld            $xr4, a4, -161
+    xvld            $xr5, a4, -193
+    xvld            $xr6, a4, -225
+    xvld            $xr7, a4, -257
 
-    xvld            $xr0, a4, -288
     addi.d          a4, a4, -256
-    xvst            $xr1, a3, -64
-    xvst            $xr2, a3, -96
+    xvst            $xr0, a5, -32
+    xvst            $xr1, a5, -64
+    xvst            $xr2, a5, -96
 
-    xvst            $xr3, a3, -128
-    xvst            $xr4, a3, -160
-    xvst            $xr5, a3, -192
-    xvst            $xr6, a3, -224
 
+    xvst            $xr3, a5, -128
+    xvst            $xr4, a5, -160
+    xvst            $xr5, a5, -192
+    xvst            $xr6, a5, -224
 
-    xvst            $xr7, a3, -256
-    addi.d          a3, a3, -256
+    xvst            $xr7, a5, -256
+    addi.d          a5, a5, -256
     bne             a4, a6, L(back_loop_256)
-L(back_long_end):
-    blt             a2, t8, L(back_end_less_128)
+L(back_lt256):
+    srli.d          t2, a2, 7
 
-    xvld            $xr1, a4, -64
-    xvld            $xr2, a4, -96
-    xvld            $xr3, a4, -128
-    addi.d          a2, a2, -128
+    beqz            t2, L(back_lt128)
+    xvld            $xr0, a4, -33
+    xvld            $xr1, a4, -65
+    xvld            $xr2, a4, -97
 
-    xvst            $xr0, a3, -32
-    xvld            $xr0, a4, -160
+    xvld            $xr3, a4, -129
+    addi.d          a2, a2, -128
     addi.d          a4, a4, -128
-    xvst            $xr1, a3, -64
+    xvst            $xr0, a5, -32
 
-    xvst            $xr2, a3, -96
-    xvst            $xr3, a3, -128
-    addi.d          a3, a3, -128
-L(back_end_less_128):
-    blt             a2, t7, L(back_end_less_64)
 
+    xvst            $xr1, a5, -64
+    xvst            $xr2, a5, -96
+    xvst            $xr3, a5, -128
+    addi.d          a5, a5, -128
 
-    xvld            $xr1, a4, -64
+L(back_lt128):
+    blt             a2, t1, L(back_lt64)
+    xvld            $xr0, a4, -33
+    xvld            $xr1, a4, -65
     addi.d          a2, a2, -64
-    xvst            $xr0, a3, -32
-    xvld            $xr0, a4, -96
 
     addi.d          a4, a4, -64
-    xvst            $xr1, a3, -64
-    addi.d          a3, a3, -64
-L(back_end_less_64):
-    blt             a2, t6, L(back_end_less_32)
-
-    xvst            $xr0, a3, -32
-    xvld            $xr0, a4, -64
-    addi.d          a3, a3, -32
-L(back_end_less_32):
-    xvld            $xr1, a1, 0
-
-    xvst            $xr0, a3, -32
-    xvst            $xr1, a0, 0
+    xvst            $xr0, a5, -32
+    xvst            $xr1, a5, -64
+    addi.d          a5, a5, -64
+
+L(back_lt64):
+    bltu            a2, t0, L(back_lt32)
+    xvld            $xr0, a4, -33
+    xvst            $xr0, a5, -32
+L(back_lt32):
+    xvst            $xr8, a0, 0
+
+
+    xvst            $xr9, a3, -31
     jr              ra
 END(MEMMOVE_NAME)
 
-- 
2.20.1

